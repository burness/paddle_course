{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paddlepaddle基本入门\n",
    "\n",
    "### 基本使用概念\n",
    "\n",
    "#### data reader\n",
    "\n",
    "paddlepaddle里面使用data reader来读取数据，主要包括三个部分：reader，reader_creator, 更复杂的creator。\n",
    "\n",
    "**reader**：reader是指从数据源返回一条一条的数据，数据源可以是普通文件、二进制文件、分布式文件系统上的文件等等，也可以就是一个随机数生成器，只要能够返回一个或者多个数据项即可；\n",
    "**reader_creator**：是一个对reader进行处理，封装的函数，对一些日常处理的基本操作封装在creator中；\n",
    "**更复杂的creator**：对creator再封装一些基本的操作，如数据映射函数、firstN、合并操作等等；\n",
    "\n",
    "reader、reader_creator、更复杂的creator从底层一步步抽样到上层解决用户需求。这里我们贴出一些代码作为分析。\n",
    "\n",
    "**reader**\n",
    "\n",
    "对普通文件的操作：\n",
    "\n",
    "    def reader():\n",
    "        f = open(path, \"r\")\n",
    "        for l in f:\n",
    "            yield l.rstrip('\\n')\n",
    "        f.close()\n",
    "    return reader\n",
    "返回一个读取每行的生成器即可，会加上去掉转行符这种基本操作；当然，也可以从np.array返回：\n",
    "\n",
    "    def reader():\n",
    "        if x.ndim < 1:\n",
    "            yield x\n",
    "\n",
    "        for e in x:\n",
    "            yield e\n",
    "\n",
    "    return reader\n",
    "又或者是从二进制文件返回：\n",
    "\n",
    "    def reader():\n",
    "        if isinstance(paths, basestring):\n",
    "            path = paths\n",
    "        else:\n",
    "            path = \",\".join(paths)\n",
    "        f = rec.reader(path)\n",
    "        while True:\n",
    "            r = f.read()\n",
    "            if r is None:\n",
    "                break\n",
    "            yield pickle.loads(r)\n",
    "        f.close()\n",
    "还有从网络返回：\n",
    "\n",
    "    def reader():\n",
    "        global pass_num\n",
    "        c.paddle_start_get_records(pass_num)\n",
    "        pass_num += 1\n",
    "\n",
    "        while True:\n",
    "            r, e = c.next_record()\n",
    "            if not r:\n",
    "                if e != -2:\n",
    "                    print \"get record error: \", e\n",
    "                break\n",
    "            yield pickle.loads(r)\n",
    "\n",
    "    return reader\n",
    "        \n",
    "**reader_creator**\n",
    "对creator详细的了解，可以读下paddlepaddle中[Paddle/python/paddle/v2/reader/creator.py](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/reader/creator.py)，贴出cloud_reader的代码作为分析：\n",
    "\n",
    "    def cloud_reader(paths, etcd_endpoints, timeout_sec=5, buf_size=64):\n",
    "        \"\"\"\n",
    "        Create a data reader that yield a record one by one from\n",
    "            the paths:\n",
    "        :paths: path of recordio files, can be a string or a string list.\n",
    "        :etcd_endpoints: the endpoints for etcd cluster\n",
    "        :returns: data reader of recordio files.\n",
    "\n",
    "        ..  code-block:: python\n",
    "            from paddle.v2.reader.creator import cloud_reader\n",
    "            etcd_endpoints = \"http://127.0.0.1:2379\"\n",
    "            trainer.train.(\n",
    "                reader=cloud_reader([\"/work/dataset/uci_housing/uci_housing*\"], etcd_endpoints),\n",
    "            )\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import cPickle as pickle\n",
    "        import paddle.v2.master as master\n",
    "        c = master.client(etcd_endpoints, timeout_sec, buf_size)\n",
    "\n",
    "        if isinstance(paths, basestring):\n",
    "            path = [paths]\n",
    "        else:\n",
    "            path = paths\n",
    "        c.set_dataset(path)\n",
    "\n",
    "        def reader():\n",
    "            global pass_num\n",
    "            c.paddle_start_get_records(pass_num)\n",
    "            pass_num += 1\n",
    "\n",
    "            while True:\n",
    "                r, e = c.next_record()\n",
    "                if not r:\n",
    "                    if e != -2:\n",
    "                        print \"get record error: \", e\n",
    "                    break\n",
    "                yield pickle.loads(r)\n",
    "\n",
    "        return reader\n",
    "\n",
    "cloud_reader这里包括客户端请求hdfs集群，然后从指定路径下的文件生成数据迭代器，有兴趣的可以看下client.py里面的代码，主要是调用`libpaddle_master.so`中的函数来完成相应的操作。\n",
    "\n",
    "\n",
    "**更复杂的creator**\n",
    "    \n",
    "更复杂的creator，可以在[decorator.py](https://github.com/PaddlePaddle/Paddle/blob/b4302bbbb85bbfd984cb2825887c133120699775/python/paddle/v2/reader/decorator.py)看到，包括map_readers, chain, compose等等。\n",
    "\n",
    "    def map_readers(func, *readers):\n",
    "        \"\"\"\n",
    "        Creates a data reader that outputs return value of function using\n",
    "        output of each data readers as arguments.\n",
    "\n",
    "        :param func: function to use. The type of func should be (Sample) => Sample\n",
    "        :type: callable\n",
    "        :param readers: readers whose outputs will be used as arguments of func.\n",
    "        :return: the created data reader.\n",
    "        :rtype: callable\n",
    "        \"\"\"\n",
    "\n",
    "        def reader():\n",
    "            rs = []\n",
    "            for r in readers:\n",
    "                rs.append(r())\n",
    "            for e in itertools.imap(func, *rs):\n",
    "                yield e\n",
    "\n",
    "        return reader\n",
    "        \n",
    "    def compose(*readers, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a data reader whose output is the combination of input readers.\n",
    "\n",
    "        If input readers output following data entries:\n",
    "        (1, 2)    3    (4, 5)\n",
    "        The composed reader will output:\n",
    "        (1, 2, 3, 4, 5)\n",
    "\n",
    "        :param readers: readers that will be composed together.\n",
    "        :param check_alignment: if True, will check if input readers are aligned\n",
    "            correctly. If False, will not check alignment and trailing outputs\n",
    "            will be discarded. Defaults to True.\n",
    "        :type check_alignment: bool\n",
    "\n",
    "        :return: the new data reader.\n",
    "\n",
    "        :raises ComposeNotAligned: outputs of readers are not aligned.\n",
    "            Will not raise when check_alignment is set to False.\n",
    "        \"\"\"\n",
    "        check_alignment = kwargs.pop('check_alignment', True)\n",
    "\n",
    "        def make_tuple(x):\n",
    "            if isinstance(x, tuple):\n",
    "                return x\n",
    "            else:\n",
    "                return (x, )\n",
    "\n",
    "        def reader():\n",
    "            rs = []\n",
    "            for r in readers:\n",
    "                rs.append(r())\n",
    "            if not check_alignment:\n",
    "                for outputs in itertools.izip(*rs):\n",
    "                    yield sum(map(make_tuple, outputs), ())\n",
    "            else:\n",
    "                for outputs in itertools.izip_longest(*rs):\n",
    "                    for o in outputs:\n",
    "                        if o is None:\n",
    "                            # None will be not be present if compose is aligned\n",
    "                            raise ComposeNotAligned(\n",
    "                                \"outputs of readers are not aligned.\")\n",
    "                    yield sum(map(make_tuple, outputs), ())\n",
    "\n",
    "        return reader\n",
    "其中，map_readers对数据流进行一个函数映射计算，批量对数据流计算计算，compose对feature数据进行合并，在模型特征工程阶段，提升很多，试想一个场景，当我们尝试不同特征组合时，如果每一次都要从数据源阶段处理好，比较麻烦，但是如果当不同的feature来源能够直接在数据读取处理时，就会方便很多，如(1,2) 3 (4,5) 组合为(1,2,3,4,5)，只需要修改需要修改的reader即可。\n",
    "\n",
    "#### layer\n",
    "\n",
    "##### data layer\n",
    "\n",
    "根据以往经验，神经网络中我们组合不同的层来搭建神经网络，paddlepaddle使用一些特定的层作为神经网络的输入：\n",
    "\n",
    "    x = paddle.layer.data(name='x', type=paddle.data_type.dense_vector(2))\n",
    "    y = paddle.layer.data(name='y', type=paddle.data_type.dense_vector(1))\n",
    "\n",
    "paddlepaddle支持不同的数据类型：\n",
    "\n",
    " - dense_vector:稠密的浮点数向量；\n",
    " - sparse_binary_vector: 稀疏二进制向量;\n",
    " - sparse_float_vector: 稀疏浮点数向量;\n",
    " - integer_value: 整数标签;\n",
    " \n",
    "然后按序列类型分为时间序列、非时间序列、子时间序列三类。\n",
    "\n",
    "数据类型的声明代码：\n",
    "\n",
    "    class InputType(object):\n",
    "        \"\"\"\n",
    "        InputType is the base class for paddle input types.\n",
    "\n",
    "        ..  note::\n",
    "\n",
    "            this is a base class, and should never be used by user.\n",
    "\n",
    "        :param dim: dimension of input. If the input is an integer, it means the\n",
    "                    value range. Otherwise, it means the size of layer.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of input. 0 means it is not a sequence. 1\n",
    "                         means it is a variable length sequence. 2 means it is a\n",
    "                         nested sequence.\n",
    "        :type seq_type: int\n",
    "        :param type: data type of input.\n",
    "        :type type: int\n",
    "        \"\"\"\n",
    "        __slots__ = ['dim', 'seq_type', 'type']\n",
    "\n",
    "        def __init__(self, dim, seq_type, tp):\n",
    "            self.dim = dim\n",
    "            self.seq_type = seq_type\n",
    "            self.type = tp\n",
    "\n",
    "        def __repr__(self):\n",
    "            \"\"\"\n",
    "            Return a human readable representation like 'InputType(dim=25921, \n",
    "                seq_type=SequenceType.NO_SEQUENCE, type=DataType.Dense)'\n",
    "            \"\"\"\n",
    "            repr_str = type(self).__name__\n",
    "            repr_str += '('\n",
    "            serialize_func_map = {\n",
    "                'dim': repr,\n",
    "                'seq_type': SequenceType.tostring,\n",
    "                'type': DataType.tostring\n",
    "            }\n",
    "            for idx, k in enumerate(self.__slots__):\n",
    "                if idx != 0:\n",
    "                    repr_str += ', '\n",
    "                repr_str += (\n",
    "                    k + '=' + serialize_func_map.get(k, repr)(getattr(self, k)))\n",
    "            repr_str += ')'\n",
    "            return repr_str\n",
    "\n",
    "    def dense_slot(dim, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Dense Array. It means the input feature is dense array with float type.\n",
    "        For example, if the input is an image with 28*28 pixels, the input of\n",
    "        Paddle neural network could be a dense vector with dimension 784 or a\n",
    "        numpy array with shape (28, 28).\n",
    "\n",
    "        For the 2-D convolution operation, each sample in one mini-batch must have\n",
    "        the similarly size in PaddlePaddle now. But, it supports variable-dimension\n",
    "        feature across mini-batch. For the variable-dimension, the param dim is not\n",
    "        used. While the data reader must yield numpy array and the data feeder will\n",
    "        set the data shape correctly.\n",
    "\n",
    "        :param dim: dimension of this vector.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of input.\n",
    "        :type seq_type: int\n",
    "        :return: An input type object.\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(dim, seq_type, DataType.Dense)\n",
    "\n",
    "    def sparse_non_value_slot(dim, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Sparse binary vector. It means the input feature is a sparse vector and the\n",
    "        every element in this vector is either zero or one.\n",
    "\n",
    "        :param dim: dimension of this vector.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of this input.\n",
    "        :type seq_type: int\n",
    "        :return: An input type object.\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(dim, seq_type, DataType.SparseNonValue)\n",
    "\n",
    "    def sparse_value_slot(dim, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Sparse vector. It means the input feature is a sparse vector. Most of the\n",
    "        elements in this vector are zero, others could be any float value.\n",
    "\n",
    "        :param dim: dimension of this vector.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of this input.\n",
    "        :type seq_type: int\n",
    "        :return: An input type object.\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(dim, seq_type, DataType.SparseValue)\n",
    "\n",
    "    def index_slot(value_range, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Data type of integer.\n",
    "\n",
    "        :param seq_type: sequence type of this input.\n",
    "        :type seq_type: int\n",
    "        :param value_range: range of this integer.\n",
    "        :type value_range: int\n",
    "        :return: An input type object\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(value_range, seq_type, DataType.Index)\n",
    "\n",
    "    dense_vector = dense_slot\n",
    "    sparse_binary_vector = sparse_non_value_slot\n",
    "    sparse_float_vector = sparse_value_slot\n",
    "    integer_value = index_slot\n",
    "\n",
    "其中DataType部分：\n",
    "\n",
    "    class SequenceType(object):\n",
    "        NO_SEQUENCE = 0\n",
    "        SEQUENCE = 1\n",
    "        SUB_SEQUENCE = 2\n",
    "\n",
    "        @classmethod\n",
    "        def tostring(cls, value):\n",
    "            for k in cls.__dict__:\n",
    "                if not k.startswith('__'):\n",
    "                    if getattr(cls, k) == value:\n",
    "                        return cls.__name__ + '.' + k\n",
    "            return 'INVALID(' + str(value) + ')'\n",
    "\n",
    "\n",
    "    # TODO(yuyang18): Add string data type here.\n",
    "    class DataType(object):\n",
    "        Dense = 0\n",
    "        SparseNonValue = 1\n",
    "        SparseValue = 2\n",
    "        Index = 3\n",
    "\n",
    "        @classmethod\n",
    "        def tostring(cls, value):\n",
    "            for k in cls.__dict__:\n",
    "                if not k.startswith('__'):\n",
    "                    if getattr(cls, k) == value:\n",
    "                        return cls.__name__ + '.' + k\n",
    "            return 'INVALID(' + str(value) + ')'\n",
    "\n",
    "##### full connected\n",
    "fc层的一个调用：`hidden1 = paddle.layer.fc(name=\"fc1\",input=contextemb, size=128, act=paddle.activation.Sigmoid(), layer_attr=paddle.attr.Extra(drop_rate=0.5), bias_attr=paddle.attr.Param(learning_rate=2), param_attr=paddle.attr.Param( initial_std=1. / math.sqrt(5 * 8), learning_rate=1, l2_rate=6e-4))`，找下paddle/python/v2/layer.py\n",
    "\n",
    "原先layer.py __all__=['data','parse_network']，只有两种类型，然后\n",
    "\n",
    "    for name in v1_layers.__all__:\n",
    "        obj = getattr(v1_layers, name)\n",
    "        new_name = __convert_name__(name)\n",
    "        if callable(obj) and __need_to_wrap__(name):\n",
    "            globals()[new_name] = __convert_to_v2__(obj, new_name, __name__)\n",
    "        else:\n",
    "            globals()[new_name] = obj\n",
    "        __all__.append(new_name)\n",
    "        \n",
    "经过基本的处理（`__convert_name__`,`__convert_to_v2__`），将v1_layers中所有支持的层转换为v2中支持的，v1_layers 在文件paddle/python/paddle/trainer_config_helpers/layers.py中，很容易找到`fc_layer`的定义：\n",
    "\n",
    "    def fc_layer(input,\n",
    "             size,\n",
    "             act=None,\n",
    "             name=None,\n",
    "             param_attr=None,\n",
    "             bias_attr=None,\n",
    "             layer_attr=None):\n",
    "        if isinstance(input, LayerOutput):\n",
    "            input = [input]\n",
    "            assert not isinstance(param_attr, collections.Sequence)\n",
    "            param_attr = [param_attr]\n",
    "        else:\n",
    "            if isinstance(param_attr, collections.Sequence):\n",
    "                assert len(input) == len(param_attr)\n",
    "            else:\n",
    "                if \"parameter_name\" in param_attr.attr and len(input) > 1:\n",
    "                    logger.fatal(\n",
    "                        \"When the name field of param_attr is manually specified \"\n",
    "                        \"and the input is a list, the param_attr should also be a \"\n",
    "                        \"list with each item being the param_attr for each input \"\n",
    "                        \"item. If only one named param_attr is provided, all the \"\n",
    "                        \"input items would share this parameter.\")\n",
    "                param_attr = [copy.deepcopy(param_attr) for _ in range(len(input))]\n",
    "\n",
    "        assert isinstance(input, collections.Sequence)\n",
    "\n",
    "        Layer(\n",
    "            inputs=[\n",
    "                Input(ipt.name, **attr.attr) for ipt, attr in zip(input, param_attr)\n",
    "            ],\n",
    "            name=name,\n",
    "            type=LayerType.FC_LAYER,\n",
    "            size=size,\n",
    "            bias=ParamAttr.to_bias(bias_attr),\n",
    "            active_type=act.name,\n",
    "            **ExtraLayerAttribute.to_kwargs(layer_attr))\n",
    "        return LayerOutput(\n",
    "            name, LayerType.FC_LAYER, input, activation=act, size=size)\n",
    "            \n",
    "对输入层做基本的处理，包括数据类型等等，然后Layer做层的配置，返回LayerOutput， Layer部分如下：\n",
    "\n",
    "    def Layer(name, type, **xargs):\n",
    "        layers = {}\n",
    "        layers.update(g_cost_map)\n",
    "        layers.update(g_layer_type_map)\n",
    "        layer_func = layers.get(type)\n",
    "        config_assert(layer_func, \"layer type '%s' not supported.\" % type)\n",
    "        return layer_func(name, **xargs)\n",
    "        \n",
    "LayerOutput类如下\n",
    "\n",
    "    class LayerOutput(object):\n",
    "        def __init__(self,\n",
    "                     name,\n",
    "                     layer_type,\n",
    "                     parents=None,\n",
    "                     activation=None,\n",
    "                     num_filters=None,\n",
    "                     img_norm_type=None,\n",
    "                     size=None,\n",
    "                     outputs=None,\n",
    "                     reverse=None):\n",
    "            assert isinstance(name, basestring)\n",
    "            assert isinstance(layer_type, basestring)\n",
    "            assert size is not None\n",
    "            assert LayerType.is_layer_type(layer_type)\n",
    "            self.name = name\n",
    "            self.full_name = MakeLayerNameInSubmodel(name)\n",
    "            self.layer_type = layer_type\n",
    "            if parents is not None and type(parents) != list:\n",
    "                parents = [parents]\n",
    "            self.parents = [] if parents is None else parents\n",
    "            self.activation = activation\n",
    "            self.num_filters = num_filters\n",
    "            self.img_norm_type = img_norm_type\n",
    "            self.size = size\n",
    "            if outputs is None:\n",
    "                outputs = ['default']\n",
    "            self.outputs = outputs\n",
    "            self.reverse = reverse\n",
    "\n",
    "        @property\n",
    "        def width(self):\n",
    "            return cp.g_layer_map[self.full_name].width\n",
    "\n",
    "        @property\n",
    "        def height(self):\n",
    "            return cp.g_layer_map[self.full_name].height\n",
    "\n",
    "        @property\n",
    "        def depth(self):\n",
    "            return cp.g_layer_map[self.full_name].depth\n",
    "\n",
    "        def set_input(self, input):\n",
    "            assert isinstance(input, LayerOutput)\n",
    "            assert self.layer_type == LayerType.MEMORY\n",
    "            SetMemoryInput(self.name, input.name)\n",
    "\n",
    "\n",
    "#### parameters\n",
    "\n",
    "Parameters类管理模型当中所有的参数，主要由`__param_conf__`, `__gradient_machines__`, `__tmp_params__`组成：\n",
    "\n",
    "    class Parameters(object):\n",
    "        def __init__(self):\n",
    "            self.__param_conf__ = OrderedDict()\n",
    "            self.__gradient_machines__ = []\n",
    "            self.__tmp_params__ = dict()\n",
    "\n",
    " - `__param_conf__`: OrderedDict类型，其中保存所有可学习的参数，且保持和创建的顺序一致，能够直接迭代`__param_conf__`来从头到尾访问所有参数；\n",
    " - `__gradient_machines_`: 用来保存paddlepaddle内部python端和c++拷贝；\n",
    " - `__tmp_params_`: dict类型，其中保存dummy parameters；\n",
    "\n",
    "Parameters内实现了`__append_config__()`, `update_param_conf()`, `keys()`, `names()`, `has_key()`, `__iter__()`, `__getter_inner()`, `__getitem__()`, `get_shape()`, `__setitem__()`, `get()`, `get_grad()`, `set()`, `append_gradient_machine()`, `serialize()`, `deserialize()`, `to_tar()`, `from_tar()`, `init_from_tar()`。 对模型参数进行基本的操作，具体可以看[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py)， 这部分代码难度不大，一些对Parameter内部成员基本的操作。\n",
    "\n",
    "如何使用Parameters：\n",
    "\n",
    "    data = paddle.layers.data(...)\n",
    "        ...\n",
    "        out = paddle.layers.fc(...)\n",
    "        parameters = paddle.parameters.create(out)\n",
    "        parameter_names = parameters.names()\n",
    "        fc_mat = parameters.get('fc')\n",
    "        print fc_mat\n",
    "        \n",
    "    def create(layers):\n",
    "        \"\"\"\n",
    "        Create parameter pool by topology.\n",
    "        :param layers:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        topology = Topology(layers)\n",
    "        pool = Parameters()\n",
    "        initializers = cp.g_parameter_initializer_map\n",
    "        for param in topology.proto().parameters:\n",
    "            pool.__append_config__(param)\n",
    "            if param.name in initializers:\n",
    "                pool[param.name] = initializers[param.name](param.name)\n",
    "        return pool\n",
    "        \n",
    "`create()`， 首先得到网络的拓扑图，然后遍历`topology.proto()`，将`paddle.proto.ParameterConfig`添加到Parameter类中，如果param_name在`g_parameter_initializer_map`中出现，则完成相应初始化， 凭个人感觉应该是比如这种参数初始化的操作，如要求mean、std来初始模型参数。\n",
    "\n",
    "#### optimizer\n",
    "\n",
    "Paddlepaddle包括`__all__ = ['Momentum', 'Adam', 'Adamax', 'AdaGrad', 'DecayedAdaGrad', 'AdaDelta','RMSProp', 'ModelAverage', 'L2Regularization']`这些优化器的父类optimizer，代码中考虑有local和remote模式，\n",
    "\n",
    "    class Optimizer(object):\n",
    "        def __init__(self, **kwargs):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            if 'batch_size' in kwargs:\n",
    "                del kwargs['batch_size']  # not important for python library.\n",
    "\n",
    "            def __impl__():\n",
    "                v1_optimizers.settings(batch_size=1, **kwargs)\n",
    "\n",
    "            self.__opt_conf_proto__ = config_parser_utils.parse_optimizer_config(\n",
    "                __impl__)\n",
    "            self.__opt_conf__ = swig_api.OptimizationConfig.createFromProto(\n",
    "                self.__opt_conf_proto__)\n",
    "\n",
    "        def enable_types(self):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            tmp = swig_api.ParameterOptimizer.create(self.__opt_conf__)\n",
    "            assert isinstance(tmp, swig_api.ParameterOptimizer)\n",
    "            return tmp.getParameterTypes()\n",
    "\n",
    "        def __create_local_updater__(self):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            return swig_api.ParameterUpdater.createLocalUpdater(self.__opt_conf__)\n",
    "\n",
    "        def __create_remote_updater__(self, pass_num, use_sparse_updater):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            return swig_api.ParameterUpdater.createRemoteUpdater(\n",
    "                self.__opt_conf__, pass_num, use_sparse_updater)\n",
    "\n",
    "        def __create_new_remote_updater__(self, pserver_spec, use_etcd):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            return swig_api.ParameterUpdater.createNewRemoteUpdater(\n",
    "                self.__opt_conf__, pserver_spec, use_etcd)\n",
    "\n",
    "        def create_updater(self, is_local, num_passes, use_sparse_updater,\n",
    "                           pserver_spec, use_etcd):\n",
    "            if is_local:\n",
    "                parameter_updater = self.__create_local_updater__()\n",
    "            else:\n",
    "                if pserver_spec is None:\n",
    "                    parameter_updater = self.__create_remote_updater__(\n",
    "                        num_passes, use_sparse_updater)\n",
    "                else:\n",
    "                    parameter_updater = self.__create_new_remote_updater__(\n",
    "                        pserver_spec, use_etcd)\n",
    "            return parameter_updater\n",
    "            \n",
    "`__init__()`部分主要用于初始化optimizer配置, `__opt_conf_proto__()`配置基本的模型参数，`__opt_conf__`表示从protobuf文件生成类然后绑定`__opt_conf_proto__()`得到的optimizer配置，具体可查看`OptimizerConfig.proto`:\n",
    " \n",
    "    syntax = \"proto2\";\n",
    "\n",
    "    option optimize_for = LITE_RUNTIME;\n",
    "\n",
    "    package paddle;\n",
    "\n",
    "    message SGDConfig {\n",
    "      // SGD\n",
    "      // momentum: float >= 0. Parameter updates momentum.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "      // nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "      optional double momentum = 21 [ default = 0.0 ];\n",
    "      optional double decay = 23 [ default = 0.0 ];\n",
    "      optional bool nesterov = 24 [ default = false ];\n",
    "    }\n",
    "\n",
    "    message AdadeltaConfig {\n",
    "      // Adadelta\n",
    "      // It is recommended to leave it at the default value.\n",
    "      // rho: float >= 0.\n",
    "      // epsilon: float >= 0. Fuzz factor.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "\n",
    "      // reference : [Adadelta - an adaptive learning rate\n",
    "      // method](http://arxiv.org/abs/1212.5701)\n",
    "      optional double rho = 33 [ default = 0.90 ];\n",
    "      optional double epsilon = 31 [ default = 1e-5 ];\n",
    "      optional double decay = 32 [ default = 0.0 ];\n",
    "    }\n",
    "\n",
    "    message AdagradConfig {\n",
    "      // Adagrad\n",
    "      // epsilon: float >= 0.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "\n",
    "      // reference : [Adaptive Subgradient Methods for Online Learning and\n",
    "      // Stochastic\n",
    "      // Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "      optional double epsilon = 41 [ default = 1e-5 ];\n",
    "      optional double decay = 42 [ default = 0.0 ];\n",
    "    }\n",
    "\n",
    "    message AdamConfig {\n",
    "      // Adaj\n",
    "      // beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "      // beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "      // epsilon: float >= 0. Fuzz factor.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "      // reference : [Adam - A Method for Stochastic\n",
    "      // Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "      optional double beta_1 = 41;\n",
    "      optional double beta_2 = 42;\n",
    "      optional double epsilon = 43;\n",
    "      optional double decay = 44;\n",
    "    }\n",
    "\n",
    "    message ConstLrConfig {\n",
    "      // learninRate Policy\n",
    "      optional double learning_rate = 1 [ default = 1.0 ];\n",
    "    }\n",
    "\n",
    "    message LinearLrConfig {\n",
    "      // learninRate Policy\n",
    "      optional double learning_rate = 1 [ default = 1.0 ];\n",
    "      optional double lr_decay_a = 2;\n",
    "      optional double lr_decay_b = 3;\n",
    "    }\n",
    "\n",
    "    message TensorProto {\n",
    "      enum DataType {\n",
    "        PADDLE_ELEMENT_TYPE_INT32 = 0;\n",
    "        PADDLE_ELEMENT_TYPE_UINT32 = 1;\n",
    "        PADDLE_ELEMENT_TYPE_INT64 = 2;\n",
    "        PADDLE_ELEMENT_TYPE_UINT64 = 3;\n",
    "        PADDLE_ELEMENT_TYPE_FLOAT32 = 4;\n",
    "        PADDLE_ELEMENT_TYPE_FLOAT64 = 5;\n",
    "      }\n",
    "      optional DataType data_type = 1;\n",
    "      repeated bytes content = 2;\n",
    "    }\n",
    "\n",
    "    message LrPolicyState {\n",
    "      // learninRate Policy\n",
    "      optional double learning_rate = 1 [ default = 1.0 ];\n",
    "      optional double lr_decay_a = 2;\n",
    "      optional double lr_decay_b = 3;\n",
    "    }\n",
    "\n",
    "    message SGDOptimizerState {\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto momentums = 2;\n",
    "    }\n",
    "\n",
    "    message AdadeltaOptimizerState {\n",
    "      // learning rate policy\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto accum_gradient = 2;\n",
    "      optional TensorProto accum_delta = 3;\n",
    "      optional TensorProto update_delta = 4;\n",
    "    }\n",
    "\n",
    "    message AdagradOptimizerState {\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto accum_gradient = 2;\n",
    "    }\n",
    "\n",
    "    message AdamOptimizerState {\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto momentums = 2;\n",
    "      optional TensorProto velocitys = 3;\n",
    "    }\n",
    "\n",
    "    message OptimizerConfig {\n",
    "      enum Optimizer {\n",
    "        SGD = 1;\n",
    "        Adadelta = 2;\n",
    "        Adagrad = 3;\n",
    "        Adam = 4;\n",
    "      }\n",
    "      optional Optimizer optimizer = 1;\n",
    "      optional SGDConfig sgd = 3;\n",
    "      optional AdadeltaConfig adadelta = 4;\n",
    "      optional AdagradConfig adagrad = 5;\n",
    "      optional AdamConfig adam = 6;\n",
    "\n",
    "      enum LrPolicy {\n",
    "        Const = 0;\n",
    "        Linear = 1;\n",
    "      }\n",
    "      optional LrPolicy lr_policy = 11;\n",
    "      optional ConstLrConfig const_lr = 12;\n",
    "      optional LinearLrConfig linear_lr = 13;\n",
    "\n",
    "      // common config of optimizer\n",
    "      // gradient clip when L2 exceeding value\n",
    "      optional double clip_norm = 101;\n",
    "      // gradient clip when L1 exceeding value\n",
    "      optional double clip_value = 102;\n",
    "    }\n",
    "另外，optimizer根据是local还是remote两种情况做相应地创建，在remote时，另外还区分是否sparse，类ParameterUpdater定义如下：\n",
    "\n",
    "    class ParameterUpdater {\n",
    "    private:\n",
    "      ParameterUpdater();\n",
    "\n",
    "    public:\n",
    "      static ParameterUpdater* createLocalUpdater(OptimizationConfig* config);\n",
    "      static ParameterUpdater* createRemoteUpdater(OptimizationConfig* config,\n",
    "                                                   int passCount,\n",
    "                                                   bool useSparseUpdater);\n",
    "      static ParameterUpdater* createNewRemoteUpdater(\n",
    "          OptimizationConfig* config,\n",
    "          const std::string pserverSpec,\n",
    "          const bool useEtcd) throw(UnsupportError);\n",
    "      ~ParameterUpdater();\n",
    "\n",
    "      /**\n",
    "       * @brief initialize Parameter Updater by GradientMachine.\n",
    "       * @param gm\n",
    "       */\n",
    "      void init(const GradientMachine& gm);\n",
    "\n",
    "      /**\n",
    "       * @brief begin of a training/testing of one pass.\n",
    "       */\n",
    "      void startPass();\n",
    "\n",
    "      /**\n",
    "       * @brief end of a traning/testing of one pass.\n",
    "       */\n",
    "      void finishPass();\n",
    "\n",
    "      /**\n",
    "       * @brief begin of a training/testing of one batch.\n",
    "       * @param data batch's size\n",
    "       * @return PassType, mostly will be training.\n",
    "       */\n",
    "      PassType startBatch(size_t batchSize);\n",
    "\n",
    "      /**\n",
    "       * @brief end of a traning/testing of one batch\n",
    "       * @param cost current batch cost.\n",
    "       */\n",
    "      void finishBatch(float cost);\n",
    "\n",
    "      /**\n",
    "       * @brief update a parameter (by local optimizer or by cluster pserver)\n",
    "       * @param param\n",
    "       */\n",
    "      void update(Parameter* param);\n",
    "\n",
    "      /**\n",
    "       * @breif only get required sparse rows by default.\n",
    "       * @param fullSize: get full matrix parameter if *fullSize* set\n",
    "       * @param apply: get PARAMETER_APPLY on pserver if *apply* set\n",
    "       */\n",
    "      void getParametersRemote(bool fullSize = false, bool apply = false);\n",
    "\n",
    "      /**\n",
    "       * @brief restore the average parameter.\n",
    "       * @note It is only used in AverageOptimizer. Restore will get the current\n",
    "       * PARAMETER_VALUE back.\n",
    "       */\n",
    "      void restore();\n",
    "\n",
    "      /**\n",
    "       * @brief apply. Store the average parameter.\n",
    "       * @note It is only used in AverageOptimizer. Apply will store the current\n",
    "       * PARAMETER_VALUE to buffer, calcaualte current Average Parameter, and save\n",
    "       * it to PARAMETER_VALUE.\n",
    "       */\n",
    "      void apply();\n",
    "\n",
    "      /**\n",
    "       * @brief catchUpWith The Regularization will be delayed in many situations(\n",
    "       * pserver, local sparse). Catch Up means catch the regularization up, apply\n",
    "       * regularization to all params.\n",
    "       */\n",
    "      void catchUpWith();\n",
    "\n",
    "    private:\n",
    "      ParameterUpdaterPrivate* m;\n",
    "    };\n",
    "\n",
    "其中createLocalUpdater\\createRemoteUpdater\\createNewRemoteUpdater实现如下：\n",
    "\n",
    "    ParameterUpdater *ParameterUpdater::createLocalUpdater(\n",
    "        OptimizationConfig *config) {\n",
    "      auto updater = new ParameterUpdater();\n",
    "      updater->m->updater.reset(\n",
    "          new paddle::SgdThreadUpdater(config->m->getConfig()));\n",
    "      return updater;\n",
    "    }\n",
    "\n",
    "    ParameterUpdater *ParameterUpdater::createNewRemoteUpdater(\n",
    "        OptimizationConfig *config,\n",
    "        const std::string pserverSpec,\n",
    "        const bool useEtcd) throw(UnsupportError) {\n",
    "    #ifndef PADDLE_WITHOUT_GOLANG\n",
    "      auto updater = new ParameterUpdater();\n",
    "      updater->m->updater.reset(new paddle::NewRemoteParameterUpdater(\n",
    "          config->m->getConfig(), pserverSpec, useEtcd));\n",
    "      return updater;\n",
    "    #else\n",
    "      throw UnsupportError(\"not compiled with WITH_GOLANG\");\n",
    "    #endif\n",
    "    }\n",
    "\n",
    "    ParameterUpdater *ParameterUpdater::createRemoteUpdater(\n",
    "        OptimizationConfig *config, int passCount, bool useSparseUpdater) {\n",
    "      auto updater = new ParameterUpdater();\n",
    "      auto remoteUpdater = new paddle::RemoteParameterUpdater(\n",
    "          config->m->getConfig(), passCount, nullptr);\n",
    "      if (useSparseUpdater) {\n",
    "        std::unique_ptr<paddle::ParameterUpdater> remoteUpdaterPtr(remoteUpdater);\n",
    "        auto sparseRemoteUpdater =\n",
    "            new paddle::SparseRemoteParameterUpdaterComposite(\n",
    "                config->m->getConfig(),\n",
    "                passCount,\n",
    "                false,\n",
    "                std::move(remoteUpdaterPtr));\n",
    "        updater->m->updater.reset(sparseRemoteUpdater);\n",
    "      } else {\n",
    "        updater->m->updater.reset(remoteUpdater);\n",
    "      }\n",
    "      return updater;\n",
    "    }\n",
    "我们这里用Momentum作为一个例子，来看看optimizer的使用：\n",
    "\n",
    "    class Momentum(Optimizer):\n",
    "        def __init__(self, momentum=None, sparse=False, **kwargs):\n",
    "            learning_method = v1_optimizers.MomentumOptimizer(\n",
    "                momentum=momentum, sparse=sparse)\n",
    "            super(Momentum, self).__init__(\n",
    "                learning_method=learning_method, **kwargs)\n",
    "                \n",
    "    class MomentumOptimizer(BaseSGDOptimizer):\n",
    "        def extra_settings(self):\n",
    "            default_momentum(self.momentum)\n",
    "\n",
    "        def to_setting_kwargs(self):\n",
    "            if self.sparse:\n",
    "                return {'learning_method': 'sparse_momentum'}\n",
    "            else:\n",
    "                return {'learning_method': 'momentum'}\n",
    "\n",
    "        def __init__(self, momentum=None, sparse=False):\n",
    "            self.momentum = momentum\n",
    "            self.sparse = sparse\n",
    "\n",
    "以上两部分代码是Momentum的使用配置，大家都知道paddlepaddl而这类框架和tf是一样的，会构建graph，这部分代码就是配置graph中的Momentum，看到这里，如何使用Momentum，包括里面的参数配置应该熟悉了，那么我们接下来深挖下，Momentum的逻辑实现，paddlepaddle所有的ops都在paddle/fluid/operators/这个目录下， 我们找到\n",
    "\n",
    "    class MomentumOp : public framework::OperatorWithKernel {\n",
    "     public:\n",
    "      using framework::OperatorWithKernel::OperatorWithKernel;\n",
    "\n",
    "     protected:\n",
    "      void InferShape(framework::InferShapeContext *ctx) const override {\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"Param\"),\n",
    "                       \"Input(param) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"Grad\"),\n",
    "                       \"Input(grad) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"Velocity\"),\n",
    "                       \"Input(velocity) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"LearningRate\"),\n",
    "                       \"Input(LearningRate) of Momentum should not be null.\");\n",
    "\n",
    "        PADDLE_ENFORCE(ctx->HasOutput(\"ParamOut\"),\n",
    "                       \"Output(ParamOut) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasOutput(\"VelocityOut\"),\n",
    "                       \"Output(VelocityOut) of Momentum should not be null.\");\n",
    "\n",
    "        auto param_dim = ctx->GetInputDim(\"Param\");\n",
    "        PADDLE_ENFORCE_EQ(\n",
    "            param_dim, ctx->GetInputDim(\"Grad\"),\n",
    "            \"Param and Grad input of MomentumOp should have the same dimension.\");\n",
    "        PADDLE_ENFORCE_EQ(\n",
    "            param_dim, ctx->GetInputDim(\"Velocity\"),\n",
    "            \"Param and Velocity of MomentumOp should have the same dimension.\");\n",
    "        PADDLE_ENFORCE_EQ(framework::product(ctx->GetInputDim(\"LearningRate\")), 1,\n",
    "                          \"Learning_rate should be a scalar\");\n",
    "\n",
    "        ctx->SetOutputDim(\"ParamOut\", param_dim);\n",
    "        ctx->SetOutputDim(\"VelocityOut\", param_dim);\n",
    "      }\n",
    "    };\n",
    "\n",
    "    class MomentumOpMaker : public framework::OpProtoAndCheckerMaker {\n",
    "     public:\n",
    "      MomentumOpMaker(OpProto *proto, OpAttrChecker *op_checker)\n",
    "          : OpProtoAndCheckerMaker(proto, op_checker) {\n",
    "        AddInput(\"Param\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input parameter that has to be updated\");\n",
    "        AddInput(\"Grad\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input gradient of the parameter\");\n",
    "        AddInput(\"Velocity\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input velocity (corresponding to the parameter) \"\n",
    "                 \"that has to be updated\");\n",
    "        AddInput(\"LearningRate\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input learning rate\");\n",
    "\n",
    "        AddOutput(\"ParamOut\",\n",
    "                  \"(Tensor) This output is updated parameter. \"\n",
    "                  \"It shared memory with Input(Param).\");\n",
    "        AddOutput(\"VelocityOut\",\n",
    "                  \"(Tensor) This output is updated velocity. \"\n",
    "                  \"It shared memory with Input(Velocity).\");\n",
    "\n",
    "        AddAttr<float>(\"mu\", \"(float) Momentum coefficient\");\n",
    "        AddAttr<bool>(\"use_nesterov\",\n",
    "                      \"(bool, default false) \"\n",
    "                      \"Use Nesterov Momentum\")\n",
    "            .SetDefault(false);\n",
    "        AddComment(R\"DOC(\n",
    "    Momentum Optimizer.\n",
    "\n",
    "    This optimizer has a flag for Nestrov Momentum.\n",
    "    The update equations are as follows:\n",
    "\n",
    "    $$\n",
    "    velocity = mu * velocity + gradient \\\\\n",
    "    if (use\\_nesterov):   \\\\\n",
    "      param = param - gradient * learning\\_rate + mu * velocity * learning\\_rate \\\\\n",
    "    else:   \\\\\n",
    "      param = param - learning\\_rate * velocity. \\\\\n",
    "    $$\n",
    "\n",
    "    )DOC\");\n",
    "      }\n",
    "    };\n",
    "    }  // namespace operators\n",
    "    }  // namespace paddle\n",
    "\n",
    "    namespace ops = paddle::operators;\n",
    "    REGISTER_OP_WITHOUT_GRADIENT(momentum, ops::MomentumOp, ops::MomentumOpMaker);\n",
    "    REGISTER_OP_CPU_KERNEL(momentum, ops::MomentumOpKernel<float>,\n",
    "                           ops::MomentumOpKernel<double>);\n",
    " \n",
    "上面用给op配置对应的输入、输出信息`momentum_op.cc`，还有一些基本的配置参数，Momentum的逻辑在`momentum_op.h`，有点奇怪，和常规的是不是有点不一样，一般来说我们会在h文件里面做基本的配置，算法逻辑写在cc文件才是，这里好像恰恰是相反的，不过逻辑还是蛮清晰的，拿到对应的input、output，然后转换成需要的格式，然后增加数据逻辑计算:\n",
    "\n",
    "    template <typename T>\n",
    "    class MomentumOpKernel : public framework::OpKernel<T> {\n",
    "     public:\n",
    "      void Compute(const framework::ExecutionContext& ctx) const override {\n",
    "        auto param_out = ctx.Output<framework::Tensor>(\"ParamOut\");\n",
    "        auto velocity_out = ctx.Output<framework::Tensor>(\"VelocityOut\");\n",
    "        auto param = ctx.Input<framework::Tensor>(\"Param\");\n",
    "        auto velocity = ctx.Input<framework::Tensor>(\"Velocity\");\n",
    "        auto grad = ctx.Input<framework::Tensor>(\"Grad\");\n",
    "        auto learning_rate = ctx.Input<framework::Tensor>(\"LearningRate\");\n",
    "\n",
    "        param_out->mutable_data<T>(ctx.GetPlace());\n",
    "        velocity_out->mutable_data<T>(ctx.GetPlace());\n",
    "\n",
    "        T mu = static_cast<T>(ctx.Attr<float>(\"mu\"));\n",
    "        bool use_nesterov = ctx.Attr<bool>(\"use_nesterov\");\n",
    "\n",
    "        auto p_out = framework::EigenVector<T>::Flatten(*param_out);\n",
    "        auto v_out = framework::EigenVector<T>::Flatten(*velocity_out);\n",
    "\n",
    "        auto p = framework::EigenVector<T>::Flatten(*param);\n",
    "        auto v = framework::EigenVector<T>::Flatten(*velocity);\n",
    "        auto g = framework::EigenVector<T>::Flatten(*grad);\n",
    "        auto* lr = learning_rate->data<T>();\n",
    "\n",
    "        v_out = v * mu + g;\n",
    "        if (use_nesterov) {\n",
    "          p_out = p - (g - v_out * mu) * lr[0];\n",
    "        } else {\n",
    "          p_out = p - lr[0] * v_out;\n",
    "        }\n",
    "      }\n",
    "\n",
    "          \n",
    "#### event handler\n",
    "event handler是指在训练过程中，用来做一些事件的处理，搜索test_train.py，里面有用法的示例：\n",
    "\n",
    "    def event_handler(event):\n",
    "        if isinstance(event, paddle.event.EndIteration):\n",
    "            if event.batch_id % 100 == 0:\n",
    "                print \"Pass %d, Batch %d, Cost %f\" % (\n",
    "                    event.pass_id, event.batch_id, event.cost)\n",
    "\n",
    "        if isinstance(event, paddle.event.EndPass):\n",
    "            if (event.pass_id + 1) % 10 == 0:\n",
    "                result = trainer.test(\n",
    "                    reader=paddle.batch(\n",
    "                        uci_housing.test(), batch_size=2),\n",
    "                    feeding={'x': 0,\n",
    "                             'y': 1})\n",
    "                print \"Test %d, %.2f\" % (event.pass_id, result.cost)\n",
    "在每一个batch之后，打印pass_id(epoch_id)，batch_id，cost等信息，每一个pass之后做一次测试集评估。\n",
    "\n",
    "    trainer.train(\n",
    "        reader=paddle.batch(\n",
    "            paddle.reader.shuffle(\n",
    "                cloud_reader(\n",
    "                    [\"/pfs/dlnel/public/dataset/uci_housing/uci_housing*\"],\n",
    "                    etcd_endpoints),\n",
    "                buf_size=500),\n",
    "            batch_size=2),\n",
    "        feeding={'x': 0,\n",
    "                 'y': 1},\n",
    "        event_handler=event_handler,\n",
    "        num_passes=30)\n",
    "        \n",
    "配置trainer.train，我们来看看相应的逻辑：\n",
    "    \n",
    "    for pass_id in xrange(num_passes):\n",
    "            event_handler(v2_event.BeginPass(pass_id))\n",
    "            pass_evaluator.start()\n",
    "            self.__parameter_updater__.startPass()\n",
    "            for batch_id, data_batch in enumerate(reader()):\n",
    "                batch_evaluator.start()\n",
    "                event_handler(\n",
    "                    v2_event.BeginIteration(\n",
    "                        pass_id=pass_id, batch_id=batch_id))\n",
    "                pass_type = self.__parameter_updater__.startBatch(\n",
    "                    len(data_batch))\n",
    "                in_args = feeder(data_batch)\n",
    "                self.__prepare_parameter__(in_args)\n",
    "                self.__gradient_machine__.forwardBackward(in_args, out_args,\n",
    "                                                          pass_type)\n",
    "                self.__gradient_machine__.eval(pass_evaluator)\n",
    "                self.__gradient_machine__.eval(batch_evaluator)\n",
    "                event_handler(\n",
    "                    v2_event.EndForwardBackward(\n",
    "                        pass_id=pass_id,\n",
    "                        batch_id=batch_id,\n",
    "                        gm=self.__gradient_machine__))\n",
    "                for each_param in self.__gradient_machine__.getNonStaticParameters(\n",
    "                ):\n",
    "                    self.__parameter_updater__.update(each_param)\n",
    "                cost_sum = out_args.sum()\n",
    "                cost = cost_sum / len(data_batch)\n",
    "                self.__parameter_updater__.finishBatch(cost)\n",
    "                batch_evaluator.finish()\n",
    "                event_handler(\n",
    "                    v2_event.EndIteration(\n",
    "                        pass_id=pass_id,\n",
    "                        batch_id=batch_id,\n",
    "                        cost=cost,\n",
    "                        evaluator=batch_evaluator,\n",
    "                        gm=self.__gradient_machine__))\n",
    "\n",
    "            self.__parameter_updater__.finishPass()\n",
    "            pass_evaluator.finish()\n",
    "            event_handler(\n",
    "                v2_event.EndPass(\n",
    "                    pass_id,\n",
    "                    evaluator=pass_evaluator,\n",
    "                    gm=self.__gradient_machine__))\n",
    "\n",
    "event_handler分别在BeginIteration\\endIterator\\BeginPass\\EndPass去分别更新batch_id\\pass_id 等信息，在event_handler中，如果是endIterator或者EndPass对象，则打印相应的信息。\n",
    "\n",
    "### 经典例子\n",
    "前面从基础部分了解了paddlepaddle， 也从代码层面看了，如何在框架中实现一些基本的逻辑功能、参数配置等等，接下来会给出两个例子：线性回归和逻辑回归。\n",
    "#### 线性回归\n",
    "\n",
    "#### 逻辑回归\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paddle.v2 as paddle\n",
    "import numpy as np\n",
    "\n",
    "paddle.init(use_gpu=False)\n",
    "\n",
    "x = paddle.layer.data(name='x', type=paddle.data_type.dense_vector(2))\n",
    "y = paddle.layer.data(name='y', type=paddle.data_type.dense_vector(1))\n",
    "y_predict = paddle.layer.fc(input=x, size=1, act=paddle.activation.Linear())\n",
    "cost = paddle.layer.square_error_cost(input=y_predict, label=y)\n",
    "\n",
    "parameters = paddle.parameters.create(cost)\n",
    "optimizer = paddle.optimizer.Momentum(momentum=0)\n",
    "trainer = paddle.trainer.SGD(cost=cost,\n",
    "                             parameters=parameters,\n",
    "                             update_equation=optimizer)\n",
    "\n",
    "def event_handler(event):\n",
    "    if isinstance(event, paddle.event.EndIteration):\n",
    "        if event.batch_id % 1 == 0:\n",
    "            print \"Pass %d, Batch %d, Cost %f\" % (event.pass_id, event.batch_id,\n",
    "                                                  event.cost)\n",
    "    # product model every 10 pass\n",
    "    if isinstance(event, paddle.event.EndPass):\n",
    "        if event.pass_id % 10 == 0:\n",
    "            with open('params_pass_%d.tar' % event.pass_id, 'w') as f:\n",
    "                trainer.save_parameter_to_tar(f)\n",
    "\n",
    "def train_reader():\n",
    "    train_x = np.array([[1, 1], [1, 2], [3, 4], [5, 2]])\n",
    "    train_y = np.array([[-2], [-3], [-7], [-7]])\n",
    "\n",
    "    def reader():\n",
    "        for i in xrange(train_y.shape[0]):\n",
    "            yield train_x[i], train_y[i]\n",
    "\n",
    "    return reader\n",
    "\n",
    "feeding = {'x': 0, 'y': 1}\n",
    "\n",
    "trainer.train(\n",
    "    reader=paddle.batch(\n",
    "        train_reader(), batch_size=1),\n",
    "    feeding=feeding,\n",
    "    event_handler=event_handler,\n",
    "    num_passes=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import paddle.v2 as paddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
