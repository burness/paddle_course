{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## paddlepaddle基本入门\n",
    "\n",
    "### 基本使用概念\n",
    "\n",
    "#### data reader\n",
    "\n",
    "paddlepaddle里面使用data reader来读取数据，主要包括三个部分：reader，reader_creator, 更复杂的creator。\n",
    "\n",
    "**reader**：reader是指从数据源返回一条一条的数据，数据源可以是普通文件、二进制文件、分布式文件系统上的文件等等，也可以就是一个随机数生成器，只要能够返回一个或者多个数据项即可；\n",
    "**reader_creator**：是一个对reader进行处理，封装的函数，对一些日常处理的基本操作封装在creator中；\n",
    "**更复杂的creator**：对creator再封装一些基本的操作，如数据映射函数、firstN、合并操作等等；\n",
    "\n",
    "reader、reader_creator、更复杂的creator从底层一步步抽样到上层解决用户需求。这里我们贴出一些代码作为分析。\n",
    "\n",
    "**reader**\n",
    "\n",
    "对普通文件的操作：\n",
    "\n",
    "    def reader():\n",
    "        f = open(path, \"r\")\n",
    "        for l in f:\n",
    "            yield l.rstrip('\\n')\n",
    "        f.close()\n",
    "    return reader\n",
    "返回一个读取每行的生成器即可，会加上去掉转行符这种基本操作；当然，也可以从np.array返回：\n",
    "\n",
    "    def reader():\n",
    "        if x.ndim < 1:\n",
    "            yield x\n",
    "\n",
    "        for e in x:\n",
    "            yield e\n",
    "\n",
    "    return reader\n",
    "又或者是从二进制文件返回：\n",
    "\n",
    "    def reader():\n",
    "        if isinstance(paths, basestring):\n",
    "            path = paths\n",
    "        else:\n",
    "            path = \",\".join(paths)\n",
    "        f = rec.reader(path)\n",
    "        while True:\n",
    "            r = f.read()\n",
    "            if r is None:\n",
    "                break\n",
    "            yield pickle.loads(r)\n",
    "        f.close()\n",
    "还有从网络返回：\n",
    "\n",
    "    def reader():\n",
    "        global pass_num\n",
    "        c.paddle_start_get_records(pass_num)\n",
    "        pass_num += 1\n",
    "\n",
    "        while True:\n",
    "            r, e = c.next_record()\n",
    "            if not r:\n",
    "                if e != -2:\n",
    "                    print \"get record error: \", e\n",
    "                break\n",
    "            yield pickle.loads(r)\n",
    "\n",
    "    return reader\n",
    "        \n",
    "**reader_creator**\n",
    "对creator详细的了解，可以读下paddlepaddle中[Paddle/python/paddle/v2/reader/creator.py](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/reader/creator.py)，贴出cloud_reader的代码作为分析：\n",
    "\n",
    "    def cloud_reader(paths, etcd_endpoints, timeout_sec=5, buf_size=64):\n",
    "        \"\"\"\n",
    "        Create a data reader that yield a record one by one from\n",
    "            the paths:\n",
    "        :paths: path of recordio files, can be a string or a string list.\n",
    "        :etcd_endpoints: the endpoints for etcd cluster\n",
    "        :returns: data reader of recordio files.\n",
    "\n",
    "        ..  code-block:: python\n",
    "            from paddle.v2.reader.creator import cloud_reader\n",
    "            etcd_endpoints = \"http://127.0.0.1:2379\"\n",
    "            trainer.train.(\n",
    "                reader=cloud_reader([\"/work/dataset/uci_housing/uci_housing*\"], etcd_endpoints),\n",
    "            )\n",
    "        \"\"\"\n",
    "        import os\n",
    "        import cPickle as pickle\n",
    "        import paddle.v2.master as master\n",
    "        c = master.client(etcd_endpoints, timeout_sec, buf_size)\n",
    "\n",
    "        if isinstance(paths, basestring):\n",
    "            path = [paths]\n",
    "        else:\n",
    "            path = paths\n",
    "        c.set_dataset(path)\n",
    "\n",
    "        def reader():\n",
    "            global pass_num\n",
    "            c.paddle_start_get_records(pass_num)\n",
    "            pass_num += 1\n",
    "\n",
    "            while True:\n",
    "                r, e = c.next_record()\n",
    "                if not r:\n",
    "                    if e != -2:\n",
    "                        print \"get record error: \", e\n",
    "                    break\n",
    "                yield pickle.loads(r)\n",
    "\n",
    "        return reader\n",
    "\n",
    "cloud_reader这里包括客户端请求hdfs集群，然后从指定路径下的文件生成数据迭代器，有兴趣的可以看下client.py里面的代码，主要是调用`libpaddle_master.so`中的函数来完成相应的操作。\n",
    "\n",
    "\n",
    "**更复杂的creator**\n",
    "    \n",
    "更复杂的creator，可以在[decorator.py](https://github.com/PaddlePaddle/Paddle/blob/b4302bbbb85bbfd984cb2825887c133120699775/python/paddle/v2/reader/decorator.py)看到，包括map_readers, chain, compose等等。\n",
    "\n",
    "    def map_readers(func, *readers):\n",
    "        \"\"\"\n",
    "        Creates a data reader that outputs return value of function using\n",
    "        output of each data readers as arguments.\n",
    "\n",
    "        :param func: function to use. The type of func should be (Sample) => Sample\n",
    "        :type: callable\n",
    "        :param readers: readers whose outputs will be used as arguments of func.\n",
    "        :return: the created data reader.\n",
    "        :rtype: callable\n",
    "        \"\"\"\n",
    "\n",
    "        def reader():\n",
    "            rs = []\n",
    "            for r in readers:\n",
    "                rs.append(r())\n",
    "            for e in itertools.imap(func, *rs):\n",
    "                yield e\n",
    "\n",
    "        return reader\n",
    "        \n",
    "    def compose(*readers, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a data reader whose output is the combination of input readers.\n",
    "\n",
    "        If input readers output following data entries:\n",
    "        (1, 2)    3    (4, 5)\n",
    "        The composed reader will output:\n",
    "        (1, 2, 3, 4, 5)\n",
    "\n",
    "        :param readers: readers that will be composed together.\n",
    "        :param check_alignment: if True, will check if input readers are aligned\n",
    "            correctly. If False, will not check alignment and trailing outputs\n",
    "            will be discarded. Defaults to True.\n",
    "        :type check_alignment: bool\n",
    "\n",
    "        :return: the new data reader.\n",
    "\n",
    "        :raises ComposeNotAligned: outputs of readers are not aligned.\n",
    "            Will not raise when check_alignment is set to False.\n",
    "        \"\"\"\n",
    "        check_alignment = kwargs.pop('check_alignment', True)\n",
    "\n",
    "        def make_tuple(x):\n",
    "            if isinstance(x, tuple):\n",
    "                return x\n",
    "            else:\n",
    "                return (x, )\n",
    "\n",
    "        def reader():\n",
    "            rs = []\n",
    "            for r in readers:\n",
    "                rs.append(r())\n",
    "            if not check_alignment:\n",
    "                for outputs in itertools.izip(*rs):\n",
    "                    yield sum(map(make_tuple, outputs), ())\n",
    "            else:\n",
    "                for outputs in itertools.izip_longest(*rs):\n",
    "                    for o in outputs:\n",
    "                        if o is None:\n",
    "                            # None will be not be present if compose is aligned\n",
    "                            raise ComposeNotAligned(\n",
    "                                \"outputs of readers are not aligned.\")\n",
    "                    yield sum(map(make_tuple, outputs), ())\n",
    "\n",
    "        return reader\n",
    "其中，map_readers对数据流进行一个函数映射计算，批量对数据流计算计算，compose对feature数据进行合并，在模型特征工程阶段，提升很多，试想一个场景，当我们尝试不同特征组合时，如果每一次都要从数据源阶段处理好，比较麻烦，但是如果当不同的feature来源能够直接在数据读取处理时，就会方便很多，如(1,2) 3 (4,5) 组合为(1,2,3,4,5)，只需要修改需要修改的reader即可。\n",
    "\n",
    "#### layer\n",
    "\n",
    "##### data layer\n",
    "\n",
    "根据以往经验，神经网络中我们组合不同的层来搭建神经网络，paddlepaddle使用一些特定的层作为神经网络的输入：\n",
    "\n",
    "    x = paddle.layer.data(name='x', type=paddle.data_type.dense_vector(2))\n",
    "    y = paddle.layer.data(name='y', type=paddle.data_type.dense_vector(1))\n",
    "\n",
    "paddlepaddle支持不同的数据类型：\n",
    "\n",
    " - dense_vector:稠密的浮点数向量；\n",
    " - sparse_binary_vector: 稀疏二进制向量;\n",
    " - sparse_float_vector: 稀疏浮点数向量;\n",
    " - integer_value: 整数标签;\n",
    " \n",
    "然后按序列类型分为时间序列、非时间序列、子时间序列三类。\n",
    "\n",
    "数据类型的声明代码：\n",
    "\n",
    "    class InputType(object):\n",
    "        \"\"\"\n",
    "        InputType is the base class for paddle input types.\n",
    "\n",
    "        ..  note::\n",
    "\n",
    "            this is a base class, and should never be used by user.\n",
    "\n",
    "        :param dim: dimension of input. If the input is an integer, it means the\n",
    "                    value range. Otherwise, it means the size of layer.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of input. 0 means it is not a sequence. 1\n",
    "                         means it is a variable length sequence. 2 means it is a\n",
    "                         nested sequence.\n",
    "        :type seq_type: int\n",
    "        :param type: data type of input.\n",
    "        :type type: int\n",
    "        \"\"\"\n",
    "        __slots__ = ['dim', 'seq_type', 'type']\n",
    "\n",
    "        def __init__(self, dim, seq_type, tp):\n",
    "            self.dim = dim\n",
    "            self.seq_type = seq_type\n",
    "            self.type = tp\n",
    "\n",
    "        def __repr__(self):\n",
    "            \"\"\"\n",
    "            Return a human readable representation like 'InputType(dim=25921, \n",
    "                seq_type=SequenceType.NO_SEQUENCE, type=DataType.Dense)'\n",
    "            \"\"\"\n",
    "            repr_str = type(self).__name__\n",
    "            repr_str += '('\n",
    "            serialize_func_map = {\n",
    "                'dim': repr,\n",
    "                'seq_type': SequenceType.tostring,\n",
    "                'type': DataType.tostring\n",
    "            }\n",
    "            for idx, k in enumerate(self.__slots__):\n",
    "                if idx != 0:\n",
    "                    repr_str += ', '\n",
    "                repr_str += (\n",
    "                    k + '=' + serialize_func_map.get(k, repr)(getattr(self, k)))\n",
    "            repr_str += ')'\n",
    "            return repr_str\n",
    "\n",
    "    def dense_slot(dim, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Dense Array. It means the input feature is dense array with float type.\n",
    "        For example, if the input is an image with 28*28 pixels, the input of\n",
    "        Paddle neural network could be a dense vector with dimension 784 or a\n",
    "        numpy array with shape (28, 28).\n",
    "\n",
    "        For the 2-D convolution operation, each sample in one mini-batch must have\n",
    "        the similarly size in PaddlePaddle now. But, it supports variable-dimension\n",
    "        feature across mini-batch. For the variable-dimension, the param dim is not\n",
    "        used. While the data reader must yield numpy array and the data feeder will\n",
    "        set the data shape correctly.\n",
    "\n",
    "        :param dim: dimension of this vector.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of input.\n",
    "        :type seq_type: int\n",
    "        :return: An input type object.\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(dim, seq_type, DataType.Dense)\n",
    "\n",
    "    def sparse_non_value_slot(dim, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Sparse binary vector. It means the input feature is a sparse vector and the\n",
    "        every element in this vector is either zero or one.\n",
    "\n",
    "        :param dim: dimension of this vector.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of this input.\n",
    "        :type seq_type: int\n",
    "        :return: An input type object.\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(dim, seq_type, DataType.SparseNonValue)\n",
    "\n",
    "    def sparse_value_slot(dim, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Sparse vector. It means the input feature is a sparse vector. Most of the\n",
    "        elements in this vector are zero, others could be any float value.\n",
    "\n",
    "        :param dim: dimension of this vector.\n",
    "        :type dim: int\n",
    "        :param seq_type: sequence type of this input.\n",
    "        :type seq_type: int\n",
    "        :return: An input type object.\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(dim, seq_type, DataType.SparseValue)\n",
    "\n",
    "    def index_slot(value_range, seq_type=SequenceType.NO_SEQUENCE):\n",
    "        \"\"\"\n",
    "        Data type of integer.\n",
    "\n",
    "        :param seq_type: sequence type of this input.\n",
    "        :type seq_type: int\n",
    "        :param value_range: range of this integer.\n",
    "        :type value_range: int\n",
    "        :return: An input type object\n",
    "        :rtype: InputType\n",
    "        \"\"\"\n",
    "        return InputType(value_range, seq_type, DataType.Index)\n",
    "\n",
    "    dense_vector = dense_slot\n",
    "    sparse_binary_vector = sparse_non_value_slot\n",
    "    sparse_float_vector = sparse_value_slot\n",
    "    integer_value = index_slot\n",
    "\n",
    "其中DataType部分：\n",
    "\n",
    "    class SequenceType(object):\n",
    "        NO_SEQUENCE = 0\n",
    "        SEQUENCE = 1\n",
    "        SUB_SEQUENCE = 2\n",
    "\n",
    "        @classmethod\n",
    "        def tostring(cls, value):\n",
    "            for k in cls.__dict__:\n",
    "                if not k.startswith('__'):\n",
    "                    if getattr(cls, k) == value:\n",
    "                        return cls.__name__ + '.' + k\n",
    "            return 'INVALID(' + str(value) + ')'\n",
    "\n",
    "\n",
    "    # TODO(yuyang18): Add string data type here.\n",
    "    class DataType(object):\n",
    "        Dense = 0\n",
    "        SparseNonValue = 1\n",
    "        SparseValue = 2\n",
    "        Index = 3\n",
    "\n",
    "        @classmethod\n",
    "        def tostring(cls, value):\n",
    "            for k in cls.__dict__:\n",
    "                if not k.startswith('__'):\n",
    "                    if getattr(cls, k) == value:\n",
    "                        return cls.__name__ + '.' + k\n",
    "            return 'INVALID(' + str(value) + ')'\n",
    "\n",
    "##### full connected\n",
    "fc层的一个调用：`hidden1 = paddle.layer.fc(name=\"fc1\",input=contextemb, size=128, act=paddle.activation.Sigmoid(), layer_attr=paddle.attr.Extra(drop_rate=0.5), bias_attr=paddle.attr.Param(learning_rate=2), param_attr=paddle.attr.Param( initial_std=1. / math.sqrt(5 * 8), learning_rate=1, l2_rate=6e-4))`，找下paddle/python/v2/layer.py\n",
    "\n",
    "原先layer.py __all__=['data','parse_network']，只有两种类型，然后\n",
    "\n",
    "    for name in v1_layers.__all__:\n",
    "        obj = getattr(v1_layers, name)\n",
    "        new_name = __convert_name__(name)\n",
    "        if callable(obj) and __need_to_wrap__(name):\n",
    "            globals()[new_name] = __convert_to_v2__(obj, new_name, __name__)\n",
    "        else:\n",
    "            globals()[new_name] = obj\n",
    "        __all__.append(new_name)\n",
    "        \n",
    "经过基本的处理（`__convert_name__`,`__convert_to_v2__`），将v1_layers中所有支持的层转换为v2中支持的，v1_layers 在文件paddle/python/paddle/trainer_config_helpers/layers.py中，很容易找到`fc_layer`的定义：\n",
    "\n",
    "    def fc_layer(input,\n",
    "             size,\n",
    "             act=None,\n",
    "             name=None,\n",
    "             param_attr=None,\n",
    "             bias_attr=None,\n",
    "             layer_attr=None):\n",
    "        if isinstance(input, LayerOutput):\n",
    "            input = [input]\n",
    "            assert not isinstance(param_attr, collections.Sequence)\n",
    "            param_attr = [param_attr]\n",
    "        else:\n",
    "            if isinstance(param_attr, collections.Sequence):\n",
    "                assert len(input) == len(param_attr)\n",
    "            else:\n",
    "                if \"parameter_name\" in param_attr.attr and len(input) > 1:\n",
    "                    logger.fatal(\n",
    "                        \"When the name field of param_attr is manually specified \"\n",
    "                        \"and the input is a list, the param_attr should also be a \"\n",
    "                        \"list with each item being the param_attr for each input \"\n",
    "                        \"item. If only one named param_attr is provided, all the \"\n",
    "                        \"input items would share this parameter.\")\n",
    "                param_attr = [copy.deepcopy(param_attr) for _ in range(len(input))]\n",
    "\n",
    "        assert isinstance(input, collections.Sequence)\n",
    "\n",
    "        Layer(\n",
    "            inputs=[\n",
    "                Input(ipt.name, **attr.attr) for ipt, attr in zip(input, param_attr)\n",
    "            ],\n",
    "            name=name,\n",
    "            type=LayerType.FC_LAYER,\n",
    "            size=size,\n",
    "            bias=ParamAttr.to_bias(bias_attr),\n",
    "            active_type=act.name,\n",
    "            **ExtraLayerAttribute.to_kwargs(layer_attr))\n",
    "        return LayerOutput(\n",
    "            name, LayerType.FC_LAYER, input, activation=act, size=size)\n",
    "            \n",
    "对输入层做基本的处理，包括数据类型等等，然后Layer做层的配置，返回LayerOutput， Layer部分如下：\n",
    "\n",
    "    def Layer(name, type, **xargs):\n",
    "        layers = {}\n",
    "        layers.update(g_cost_map)\n",
    "        layers.update(g_layer_type_map)\n",
    "        layer_func = layers.get(type)\n",
    "        config_assert(layer_func, \"layer type '%s' not supported.\" % type)\n",
    "        return layer_func(name, **xargs)\n",
    "        \n",
    "LayerOutput类如下\n",
    "\n",
    "    class LayerOutput(object):\n",
    "        def __init__(self,\n",
    "                     name,\n",
    "                     layer_type,\n",
    "                     parents=None,\n",
    "                     activation=None,\n",
    "                     num_filters=None,\n",
    "                     img_norm_type=None,\n",
    "                     size=None,\n",
    "                     outputs=None,\n",
    "                     reverse=None):\n",
    "            assert isinstance(name, basestring)\n",
    "            assert isinstance(layer_type, basestring)\n",
    "            assert size is not None\n",
    "            assert LayerType.is_layer_type(layer_type)\n",
    "            self.name = name\n",
    "            self.full_name = MakeLayerNameInSubmodel(name)\n",
    "            self.layer_type = layer_type\n",
    "            if parents is not None and type(parents) != list:\n",
    "                parents = [parents]\n",
    "            self.parents = [] if parents is None else parents\n",
    "            self.activation = activation\n",
    "            self.num_filters = num_filters\n",
    "            self.img_norm_type = img_norm_type\n",
    "            self.size = size\n",
    "            if outputs is None:\n",
    "                outputs = ['default']\n",
    "            self.outputs = outputs\n",
    "            self.reverse = reverse\n",
    "\n",
    "        @property\n",
    "        def width(self):\n",
    "            return cp.g_layer_map[self.full_name].width\n",
    "\n",
    "        @property\n",
    "        def height(self):\n",
    "            return cp.g_layer_map[self.full_name].height\n",
    "\n",
    "        @property\n",
    "        def depth(self):\n",
    "            return cp.g_layer_map[self.full_name].depth\n",
    "\n",
    "        def set_input(self, input):\n",
    "            assert isinstance(input, LayerOutput)\n",
    "            assert self.layer_type == LayerType.MEMORY\n",
    "            SetMemoryInput(self.name, input.name)\n",
    "\n",
    "\n",
    "#### parameters\n",
    "\n",
    "Parameters类管理模型当中所有的参数，主要由`__param_conf__`, `__gradient_machines__`, `__tmp_params__`组成：\n",
    "\n",
    "    class Parameters(object):\n",
    "        def __init__(self):\n",
    "            self.__param_conf__ = OrderedDict()\n",
    "            self.__gradient_machines__ = []\n",
    "            self.__tmp_params__ = dict()\n",
    "\n",
    " - `__param_conf__`: OrderedDict类型，其中保存所有可学习的参数，且保持和创建的顺序一致，能够直接迭代`__param_conf__`来从头到尾访问所有参数；\n",
    " - `__gradient_machines_`: 用来保存paddlepaddle内部python端和c++拷贝；\n",
    " - `__tmp_params_`: dict类型，其中保存dummy parameters；\n",
    "\n",
    "Parameters内实现了`__append_config__()`, `update_param_conf()`, `keys()`, `names()`, `has_key()`, `__iter__()`, `__getter_inner()`, `__getitem__()`, `get_shape()`, `__setitem__()`, `get()`, `get_grad()`, `set()`, `append_gradient_machine()`, `serialize()`, `deserialize()`, `to_tar()`, `from_tar()`, `init_from_tar()`。 对模型参数进行基本的操作，具体可以看[https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py](https://github.com/PaddlePaddle/Paddle/blob/develop/python/paddle/v2/parameters.py)， 这部分代码难度不大，一些对Parameter内部成员基本的操作。\n",
    "\n",
    "如何使用Parameters：\n",
    "\n",
    "    data = paddle.layers.data(...)\n",
    "        ...\n",
    "        out = paddle.layers.fc(...)\n",
    "        parameters = paddle.parameters.create(out)\n",
    "        parameter_names = parameters.names()\n",
    "        fc_mat = parameters.get('fc')\n",
    "        print fc_mat\n",
    "        \n",
    "    def create(layers):\n",
    "        \"\"\"\n",
    "        Create parameter pool by topology.\n",
    "        :param layers:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        topology = Topology(layers)\n",
    "        pool = Parameters()\n",
    "        initializers = cp.g_parameter_initializer_map\n",
    "        for param in topology.proto().parameters:\n",
    "            pool.__append_config__(param)\n",
    "            if param.name in initializers:\n",
    "                pool[param.name] = initializers[param.name](param.name)\n",
    "        return pool\n",
    "        \n",
    "`create()`， 首先得到网络的拓扑图，然后遍历`topology.proto()`，将`paddle.proto.ParameterConfig`添加到Parameter类中，如果param_name在`g_parameter_initializer_map`中出现，则完成相应初始化， 凭个人感觉应该是比如这种参数初始化的操作，如要求mean、std来初始模型参数。\n",
    "\n",
    "#### optimizer\n",
    "\n",
    "Paddlepaddle包括`__all__ = ['Momentum', 'Adam', 'Adamax', 'AdaGrad', 'DecayedAdaGrad', 'AdaDelta','RMSProp', 'ModelAverage', 'L2Regularization']`这些优化器的父类optimizer，代码中考虑有local和remote模式，\n",
    "\n",
    "    class Optimizer(object):\n",
    "        def __init__(self, **kwargs):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            if 'batch_size' in kwargs:\n",
    "                del kwargs['batch_size']  # not important for python library.\n",
    "\n",
    "            def __impl__():\n",
    "                v1_optimizers.settings(batch_size=1, **kwargs)\n",
    "\n",
    "            self.__opt_conf_proto__ = config_parser_utils.parse_optimizer_config(\n",
    "                __impl__)\n",
    "            self.__opt_conf__ = swig_api.OptimizationConfig.createFromProto(\n",
    "                self.__opt_conf_proto__)\n",
    "\n",
    "        def enable_types(self):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            tmp = swig_api.ParameterOptimizer.create(self.__opt_conf__)\n",
    "            assert isinstance(tmp, swig_api.ParameterOptimizer)\n",
    "            return tmp.getParameterTypes()\n",
    "\n",
    "        def __create_local_updater__(self):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            return swig_api.ParameterUpdater.createLocalUpdater(self.__opt_conf__)\n",
    "\n",
    "        def __create_remote_updater__(self, pass_num, use_sparse_updater):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            return swig_api.ParameterUpdater.createRemoteUpdater(\n",
    "                self.__opt_conf__, pass_num, use_sparse_updater)\n",
    "\n",
    "        def __create_new_remote_updater__(self, pserver_spec, use_etcd):\n",
    "            import py_paddle.swig_paddle as swig_api\n",
    "            return swig_api.ParameterUpdater.createNewRemoteUpdater(\n",
    "                self.__opt_conf__, pserver_spec, use_etcd)\n",
    "\n",
    "        def create_updater(self, is_local, num_passes, use_sparse_updater,\n",
    "                           pserver_spec, use_etcd):\n",
    "            if is_local:\n",
    "                parameter_updater = self.__create_local_updater__()\n",
    "            else:\n",
    "                if pserver_spec is None:\n",
    "                    parameter_updater = self.__create_remote_updater__(\n",
    "                        num_passes, use_sparse_updater)\n",
    "                else:\n",
    "                    parameter_updater = self.__create_new_remote_updater__(\n",
    "                        pserver_spec, use_etcd)\n",
    "            return parameter_updater\n",
    "            \n",
    "`__init__()`部分主要用于初始化optimizer配置, `__opt_conf_proto__()`配置基本的模型参数，`__opt_conf__`表示从protobuf文件生成类然后绑定`__opt_conf_proto__()`得到的optimizer配置，具体可查看`OptimizerConfig.proto`:\n",
    " \n",
    "    syntax = \"proto2\";\n",
    "\n",
    "    option optimize_for = LITE_RUNTIME;\n",
    "\n",
    "    package paddle;\n",
    "\n",
    "    message SGDConfig {\n",
    "      // SGD\n",
    "      // momentum: float >= 0. Parameter updates momentum.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "      // nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "      optional double momentum = 21 [ default = 0.0 ];\n",
    "      optional double decay = 23 [ default = 0.0 ];\n",
    "      optional bool nesterov = 24 [ default = false ];\n",
    "    }\n",
    "\n",
    "    message AdadeltaConfig {\n",
    "      // Adadelta\n",
    "      // It is recommended to leave it at the default value.\n",
    "      // rho: float >= 0.\n",
    "      // epsilon: float >= 0. Fuzz factor.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "\n",
    "      // reference : [Adadelta - an adaptive learning rate\n",
    "      // method](http://arxiv.org/abs/1212.5701)\n",
    "      optional double rho = 33 [ default = 0.90 ];\n",
    "      optional double epsilon = 31 [ default = 1e-5 ];\n",
    "      optional double decay = 32 [ default = 0.0 ];\n",
    "    }\n",
    "\n",
    "    message AdagradConfig {\n",
    "      // Adagrad\n",
    "      // epsilon: float >= 0.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "\n",
    "      // reference : [Adaptive Subgradient Methods for Online Learning and\n",
    "      // Stochastic\n",
    "      // Optimization](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    "      optional double epsilon = 41 [ default = 1e-5 ];\n",
    "      optional double decay = 42 [ default = 0.0 ];\n",
    "    }\n",
    "\n",
    "    message AdamConfig {\n",
    "      // Adaj\n",
    "      // beta_1: float, 0 < beta < 1. Generally close to 1.\n",
    "      // beta_2: float, 0 < beta < 1. Generally close to 1.\n",
    "      // epsilon: float >= 0. Fuzz factor.\n",
    "      // decay: float >= 0. Learning rate decay over each update.\n",
    "      // reference : [Adam - A Method for Stochastic\n",
    "      // Optimization](http://arxiv.org/abs/1412.6980v8)\n",
    "      optional double beta_1 = 41;\n",
    "      optional double beta_2 = 42;\n",
    "      optional double epsilon = 43;\n",
    "      optional double decay = 44;\n",
    "    }\n",
    "\n",
    "    message ConstLrConfig {\n",
    "      // learninRate Policy\n",
    "      optional double learning_rate = 1 [ default = 1.0 ];\n",
    "    }\n",
    "\n",
    "    message LinearLrConfig {\n",
    "      // learninRate Policy\n",
    "      optional double learning_rate = 1 [ default = 1.0 ];\n",
    "      optional double lr_decay_a = 2;\n",
    "      optional double lr_decay_b = 3;\n",
    "    }\n",
    "\n",
    "    message TensorProto {\n",
    "      enum DataType {\n",
    "        PADDLE_ELEMENT_TYPE_INT32 = 0;\n",
    "        PADDLE_ELEMENT_TYPE_UINT32 = 1;\n",
    "        PADDLE_ELEMENT_TYPE_INT64 = 2;\n",
    "        PADDLE_ELEMENT_TYPE_UINT64 = 3;\n",
    "        PADDLE_ELEMENT_TYPE_FLOAT32 = 4;\n",
    "        PADDLE_ELEMENT_TYPE_FLOAT64 = 5;\n",
    "      }\n",
    "      optional DataType data_type = 1;\n",
    "      repeated bytes content = 2;\n",
    "    }\n",
    "\n",
    "    message LrPolicyState {\n",
    "      // learninRate Policy\n",
    "      optional double learning_rate = 1 [ default = 1.0 ];\n",
    "      optional double lr_decay_a = 2;\n",
    "      optional double lr_decay_b = 3;\n",
    "    }\n",
    "\n",
    "    message SGDOptimizerState {\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto momentums = 2;\n",
    "    }\n",
    "\n",
    "    message AdadeltaOptimizerState {\n",
    "      // learning rate policy\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto accum_gradient = 2;\n",
    "      optional TensorProto accum_delta = 3;\n",
    "      optional TensorProto update_delta = 4;\n",
    "    }\n",
    "\n",
    "    message AdagradOptimizerState {\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto accum_gradient = 2;\n",
    "    }\n",
    "\n",
    "    message AdamOptimizerState {\n",
    "      optional LrPolicyState lr_state = 101;\n",
    "      optional double num_sample_passed = 104;\n",
    "      // state\n",
    "      optional TensorProto parameter = 1;\n",
    "      optional TensorProto momentums = 2;\n",
    "      optional TensorProto velocitys = 3;\n",
    "    }\n",
    "\n",
    "    message OptimizerConfig {\n",
    "      enum Optimizer {\n",
    "        SGD = 1;\n",
    "        Adadelta = 2;\n",
    "        Adagrad = 3;\n",
    "        Adam = 4;\n",
    "      }\n",
    "      optional Optimizer optimizer = 1;\n",
    "      optional SGDConfig sgd = 3;\n",
    "      optional AdadeltaConfig adadelta = 4;\n",
    "      optional AdagradConfig adagrad = 5;\n",
    "      optional AdamConfig adam = 6;\n",
    "\n",
    "      enum LrPolicy {\n",
    "        Const = 0;\n",
    "        Linear = 1;\n",
    "      }\n",
    "      optional LrPolicy lr_policy = 11;\n",
    "      optional ConstLrConfig const_lr = 12;\n",
    "      optional LinearLrConfig linear_lr = 13;\n",
    "\n",
    "      // common config of optimizer\n",
    "      // gradient clip when L2 exceeding value\n",
    "      optional double clip_norm = 101;\n",
    "      // gradient clip when L1 exceeding value\n",
    "      optional double clip_value = 102;\n",
    "    }\n",
    "另外，optimizer根据是local还是remote两种情况做相应地创建，在remote时，另外还区分是否sparse，类ParameterUpdater定义如下：\n",
    "\n",
    "    class ParameterUpdater {\n",
    "    private:\n",
    "      ParameterUpdater();\n",
    "\n",
    "    public:\n",
    "      static ParameterUpdater* createLocalUpdater(OptimizationConfig* config);\n",
    "      static ParameterUpdater* createRemoteUpdater(OptimizationConfig* config,\n",
    "                                                   int passCount,\n",
    "                                                   bool useSparseUpdater);\n",
    "      static ParameterUpdater* createNewRemoteUpdater(\n",
    "          OptimizationConfig* config,\n",
    "          const std::string pserverSpec,\n",
    "          const bool useEtcd) throw(UnsupportError);\n",
    "      ~ParameterUpdater();\n",
    "\n",
    "      /**\n",
    "       * @brief initialize Parameter Updater by GradientMachine.\n",
    "       * @param gm\n",
    "       */\n",
    "      void init(const GradientMachine& gm);\n",
    "\n",
    "      /**\n",
    "       * @brief begin of a training/testing of one pass.\n",
    "       */\n",
    "      void startPass();\n",
    "\n",
    "      /**\n",
    "       * @brief end of a traning/testing of one pass.\n",
    "       */\n",
    "      void finishPass();\n",
    "\n",
    "      /**\n",
    "       * @brief begin of a training/testing of one batch.\n",
    "       * @param data batch's size\n",
    "       * @return PassType, mostly will be training.\n",
    "       */\n",
    "      PassType startBatch(size_t batchSize);\n",
    "\n",
    "      /**\n",
    "       * @brief end of a traning/testing of one batch\n",
    "       * @param cost current batch cost.\n",
    "       */\n",
    "      void finishBatch(float cost);\n",
    "\n",
    "      /**\n",
    "       * @brief update a parameter (by local optimizer or by cluster pserver)\n",
    "       * @param param\n",
    "       */\n",
    "      void update(Parameter* param);\n",
    "\n",
    "      /**\n",
    "       * @breif only get required sparse rows by default.\n",
    "       * @param fullSize: get full matrix parameter if *fullSize* set\n",
    "       * @param apply: get PARAMETER_APPLY on pserver if *apply* set\n",
    "       */\n",
    "      void getParametersRemote(bool fullSize = false, bool apply = false);\n",
    "\n",
    "      /**\n",
    "       * @brief restore the average parameter.\n",
    "       * @note It is only used in AverageOptimizer. Restore will get the current\n",
    "       * PARAMETER_VALUE back.\n",
    "       */\n",
    "      void restore();\n",
    "\n",
    "      /**\n",
    "       * @brief apply. Store the average parameter.\n",
    "       * @note It is only used in AverageOptimizer. Apply will store the current\n",
    "       * PARAMETER_VALUE to buffer, calcaualte current Average Parameter, and save\n",
    "       * it to PARAMETER_VALUE.\n",
    "       */\n",
    "      void apply();\n",
    "\n",
    "      /**\n",
    "       * @brief catchUpWith The Regularization will be delayed in many situations(\n",
    "       * pserver, local sparse). Catch Up means catch the regularization up, apply\n",
    "       * regularization to all params.\n",
    "       */\n",
    "      void catchUpWith();\n",
    "\n",
    "    private:\n",
    "      ParameterUpdaterPrivate* m;\n",
    "    };\n",
    "\n",
    "其中createLocalUpdater\\createRemoteUpdater\\createNewRemoteUpdater实现如下：\n",
    "\n",
    "    ParameterUpdater *ParameterUpdater::createLocalUpdater(\n",
    "        OptimizationConfig *config) {\n",
    "      auto updater = new ParameterUpdater();\n",
    "      updater->m->updater.reset(\n",
    "          new paddle::SgdThreadUpdater(config->m->getConfig()));\n",
    "      return updater;\n",
    "    }\n",
    "\n",
    "    ParameterUpdater *ParameterUpdater::createNewRemoteUpdater(\n",
    "        OptimizationConfig *config,\n",
    "        const std::string pserverSpec,\n",
    "        const bool useEtcd) throw(UnsupportError) {\n",
    "    #ifndef PADDLE_WITHOUT_GOLANG\n",
    "      auto updater = new ParameterUpdater();\n",
    "      updater->m->updater.reset(new paddle::NewRemoteParameterUpdater(\n",
    "          config->m->getConfig(), pserverSpec, useEtcd));\n",
    "      return updater;\n",
    "    #else\n",
    "      throw UnsupportError(\"not compiled with WITH_GOLANG\");\n",
    "    #endif\n",
    "    }\n",
    "\n",
    "    ParameterUpdater *ParameterUpdater::createRemoteUpdater(\n",
    "        OptimizationConfig *config, int passCount, bool useSparseUpdater) {\n",
    "      auto updater = new ParameterUpdater();\n",
    "      auto remoteUpdater = new paddle::RemoteParameterUpdater(\n",
    "          config->m->getConfig(), passCount, nullptr);\n",
    "      if (useSparseUpdater) {\n",
    "        std::unique_ptr<paddle::ParameterUpdater> remoteUpdaterPtr(remoteUpdater);\n",
    "        auto sparseRemoteUpdater =\n",
    "            new paddle::SparseRemoteParameterUpdaterComposite(\n",
    "                config->m->getConfig(),\n",
    "                passCount,\n",
    "                false,\n",
    "                std::move(remoteUpdaterPtr));\n",
    "        updater->m->updater.reset(sparseRemoteUpdater);\n",
    "      } else {\n",
    "        updater->m->updater.reset(remoteUpdater);\n",
    "      }\n",
    "      return updater;\n",
    "    }\n",
    "我们这里用Momentum作为一个例子，来看看optimizer的使用：\n",
    "\n",
    "    class Momentum(Optimizer):\n",
    "        def __init__(self, momentum=None, sparse=False, **kwargs):\n",
    "            learning_method = v1_optimizers.MomentumOptimizer(\n",
    "                momentum=momentum, sparse=sparse)\n",
    "            super(Momentum, self).__init__(\n",
    "                learning_method=learning_method, **kwargs)\n",
    "                \n",
    "    class MomentumOptimizer(BaseSGDOptimizer):\n",
    "        def extra_settings(self):\n",
    "            default_momentum(self.momentum)\n",
    "\n",
    "        def to_setting_kwargs(self):\n",
    "            if self.sparse:\n",
    "                return {'learning_method': 'sparse_momentum'}\n",
    "            else:\n",
    "                return {'learning_method': 'momentum'}\n",
    "\n",
    "        def __init__(self, momentum=None, sparse=False):\n",
    "            self.momentum = momentum\n",
    "            self.sparse = sparse\n",
    "\n",
    "以上两部分代码是Momentum的使用配置，大家都知道paddlepaddl而这类框架和tf是一样的，会构建graph，这部分代码就是配置graph中的Momentum，看到这里，如何使用Momentum，包括里面的参数配置应该熟悉了，那么我们接下来深挖下，Momentum的逻辑实现，paddlepaddle所有的ops都在paddle/fluid/operators/这个目录下， 我们找到\n",
    "\n",
    "    class MomentumOp : public framework::OperatorWithKernel {\n",
    "     public:\n",
    "      using framework::OperatorWithKernel::OperatorWithKernel;\n",
    "\n",
    "     protected:\n",
    "      void InferShape(framework::InferShapeContext *ctx) const override {\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"Param\"),\n",
    "                       \"Input(param) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"Grad\"),\n",
    "                       \"Input(grad) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"Velocity\"),\n",
    "                       \"Input(velocity) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasInput(\"LearningRate\"),\n",
    "                       \"Input(LearningRate) of Momentum should not be null.\");\n",
    "\n",
    "        PADDLE_ENFORCE(ctx->HasOutput(\"ParamOut\"),\n",
    "                       \"Output(ParamOut) of Momentum should not be null.\");\n",
    "        PADDLE_ENFORCE(ctx->HasOutput(\"VelocityOut\"),\n",
    "                       \"Output(VelocityOut) of Momentum should not be null.\");\n",
    "\n",
    "        auto param_dim = ctx->GetInputDim(\"Param\");\n",
    "        PADDLE_ENFORCE_EQ(\n",
    "            param_dim, ctx->GetInputDim(\"Grad\"),\n",
    "            \"Param and Grad input of MomentumOp should have the same dimension.\");\n",
    "        PADDLE_ENFORCE_EQ(\n",
    "            param_dim, ctx->GetInputDim(\"Velocity\"),\n",
    "            \"Param and Velocity of MomentumOp should have the same dimension.\");\n",
    "        PADDLE_ENFORCE_EQ(framework::product(ctx->GetInputDim(\"LearningRate\")), 1,\n",
    "                          \"Learning_rate should be a scalar\");\n",
    "\n",
    "        ctx->SetOutputDim(\"ParamOut\", param_dim);\n",
    "        ctx->SetOutputDim(\"VelocityOut\", param_dim);\n",
    "      }\n",
    "    };\n",
    "\n",
    "    class MomentumOpMaker : public framework::OpProtoAndCheckerMaker {\n",
    "     public:\n",
    "      MomentumOpMaker(OpProto *proto, OpAttrChecker *op_checker)\n",
    "          : OpProtoAndCheckerMaker(proto, op_checker) {\n",
    "        AddInput(\"Param\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input parameter that has to be updated\");\n",
    "        AddInput(\"Grad\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input gradient of the parameter\");\n",
    "        AddInput(\"Velocity\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input velocity (corresponding to the parameter) \"\n",
    "                 \"that has to be updated\");\n",
    "        AddInput(\"LearningRate\",\n",
    "                 \"(Tensor, default Tensor<float>) \"\n",
    "                 \"Input learning rate\");\n",
    "\n",
    "        AddOutput(\"ParamOut\",\n",
    "                  \"(Tensor) This output is updated parameter. \"\n",
    "                  \"It shared memory with Input(Param).\");\n",
    "        AddOutput(\"VelocityOut\",\n",
    "                  \"(Tensor) This output is updated velocity. \"\n",
    "                  \"It shared memory with Input(Velocity).\");\n",
    "\n",
    "        AddAttr<float>(\"mu\", \"(float) Momentum coefficient\");\n",
    "        AddAttr<bool>(\"use_nesterov\",\n",
    "                      \"(bool, default false) \"\n",
    "                      \"Use Nesterov Momentum\")\n",
    "            .SetDefault(false);\n",
    "        AddComment(R\"DOC(\n",
    "    Momentum Optimizer.\n",
    "\n",
    "    This optimizer has a flag for Nestrov Momentum.\n",
    "    The update equations are as follows:\n",
    "\n",
    "    $$\n",
    "    velocity = mu * velocity + gradient \\\\\n",
    "    if (use\\_nesterov):   \\\\\n",
    "      param = param - gradient * learning\\_rate + mu * velocity * learning\\_rate \\\\\n",
    "    else:   \\\\\n",
    "      param = param - learning\\_rate * velocity. \\\\\n",
    "    $$\n",
    "\n",
    "    )DOC\");\n",
    "      }\n",
    "    };\n",
    "    }  // namespace operators\n",
    "    }  // namespace paddle\n",
    "\n",
    "    namespace ops = paddle::operators;\n",
    "    REGISTER_OP_WITHOUT_GRADIENT(momentum, ops::MomentumOp, ops::MomentumOpMaker);\n",
    "    REGISTER_OP_CPU_KERNEL(momentum, ops::MomentumOpKernel<float>,\n",
    "                           ops::MomentumOpKernel<double>);\n",
    " \n",
    "上面用给op配置对应的输入、输出信息`momentum_op.cc`，还有一些基本的配置参数，Momentum的逻辑在`momentum_op.h`，有点奇怪，和常规的是不是有点不一样，一般来说我们会在h文件里面做基本的配置，算法逻辑写在cc文件才是，这里好像恰恰是相反的，不过逻辑还是蛮清晰的，拿到对应的input、output，然后转换成需要的格式，然后增加数据逻辑计算:\n",
    "\n",
    "    template <typename T>\n",
    "    class MomentumOpKernel : public framework::OpKernel<T> {\n",
    "     public:\n",
    "      void Compute(const framework::ExecutionContext& ctx) const override {\n",
    "        auto param_out = ctx.Output<framework::Tensor>(\"ParamOut\");\n",
    "        auto velocity_out = ctx.Output<framework::Tensor>(\"VelocityOut\");\n",
    "        auto param = ctx.Input<framework::Tensor>(\"Param\");\n",
    "        auto velocity = ctx.Input<framework::Tensor>(\"Velocity\");\n",
    "        auto grad = ctx.Input<framework::Tensor>(\"Grad\");\n",
    "        auto learning_rate = ctx.Input<framework::Tensor>(\"LearningRate\");\n",
    "\n",
    "        param_out->mutable_data<T>(ctx.GetPlace());\n",
    "        velocity_out->mutable_data<T>(ctx.GetPlace());\n",
    "\n",
    "        T mu = static_cast<T>(ctx.Attr<float>(\"mu\"));\n",
    "        bool use_nesterov = ctx.Attr<bool>(\"use_nesterov\");\n",
    "\n",
    "        auto p_out = framework::EigenVector<T>::Flatten(*param_out);\n",
    "        auto v_out = framework::EigenVector<T>::Flatten(*velocity_out);\n",
    "\n",
    "        auto p = framework::EigenVector<T>::Flatten(*param);\n",
    "        auto v = framework::EigenVector<T>::Flatten(*velocity);\n",
    "        auto g = framework::EigenVector<T>::Flatten(*grad);\n",
    "        auto* lr = learning_rate->data<T>();\n",
    "\n",
    "        v_out = v * mu + g;\n",
    "        if (use_nesterov) {\n",
    "          p_out = p - (g - v_out * mu) * lr[0];\n",
    "        } else {\n",
    "          p_out = p - lr[0] * v_out;\n",
    "        }\n",
    "      }\n",
    "\n",
    "          \n",
    "#### event handler\n",
    "event handler是指在训练过程中，用来做一些事件的处理，搜索test_train.py，里面有用法的示例：\n",
    "\n",
    "    def event_handler(event):\n",
    "        if isinstance(event, paddle.event.EndIteration):\n",
    "            if event.batch_id % 100 == 0:\n",
    "                print \"Pass %d, Batch %d, Cost %f\" % (\n",
    "                    event.pass_id, event.batch_id, event.cost)\n",
    "\n",
    "        if isinstance(event, paddle.event.EndPass):\n",
    "            if (event.pass_id + 1) % 10 == 0:\n",
    "                result = trainer.test(\n",
    "                    reader=paddle.batch(\n",
    "                        uci_housing.test(), batch_size=2),\n",
    "                    feeding={'x': 0,\n",
    "                             'y': 1})\n",
    "                print \"Test %d, %.2f\" % (event.pass_id, result.cost)\n",
    "                \n",
    "在每一个batch之后，打印pass_id(epoch_id)，batch_id，cost等信息，每一个pass之后做一次测试集评估。\n",
    "\n",
    "    trainer.train(\n",
    "        reader=paddle.batch(\n",
    "            paddle.reader.shuffle(\n",
    "                cloud_reader(\n",
    "                    [\"/pfs/dlnel/public/dataset/uci_housing/uci_housing*\"],\n",
    "                    etcd_endpoints),\n",
    "                buf_size=500),\n",
    "            batch_size=2),\n",
    "        feeding={'x': 0,\n",
    "                 'y': 1},\n",
    "        event_handler=event_handler,\n",
    "        num_passes=30)\n",
    "        \n",
    "配置trainer.train，我们来看看相应的逻辑：\n",
    "    \n",
    "    for pass_id in xrange(num_passes):\n",
    "            event_handler(v2_event.BeginPass(pass_id))\n",
    "            pass_evaluator.start()\n",
    "            self.__parameter_updater__.startPass()\n",
    "            for batch_id, data_batch in enumerate(reader()):\n",
    "                batch_evaluator.start()\n",
    "                event_handler(\n",
    "                    v2_event.BeginIteration(\n",
    "                        pass_id=pass_id, batch_id=batch_id))\n",
    "                pass_type = self.__parameter_updater__.startBatch(\n",
    "                    len(data_batch))\n",
    "                in_args = feeder(data_batch)\n",
    "                self.__prepare_parameter__(in_args)\n",
    "                self.__gradient_machine__.forwardBackward(in_args, out_args,\n",
    "                                                          pass_type)\n",
    "                self.__gradient_machine__.eval(pass_evaluator)\n",
    "                self.__gradient_machine__.eval(batch_evaluator)\n",
    "                event_handler(\n",
    "                    v2_event.EndForwardBackward(\n",
    "                        pass_id=pass_id,\n",
    "                        batch_id=batch_id,\n",
    "                        gm=self.__gradient_machine__))\n",
    "                for each_param in self.__gradient_machine__.getNonStaticParameters(\n",
    "                ):\n",
    "                    self.__parameter_updater__.update(each_param)\n",
    "                cost_sum = out_args.sum()\n",
    "                cost = cost_sum / len(data_batch)\n",
    "                self.__parameter_updater__.finishBatch(cost)\n",
    "                batch_evaluator.finish()\n",
    "                event_handler(\n",
    "                    v2_event.EndIteration(\n",
    "                        pass_id=pass_id,\n",
    "                        batch_id=batch_id,\n",
    "                        cost=cost,\n",
    "                        evaluator=batch_evaluator,\n",
    "                        gm=self.__gradient_machine__))\n",
    "\n",
    "            self.__parameter_updater__.finishPass()\n",
    "            pass_evaluator.finish()\n",
    "            event_handler(\n",
    "                v2_event.EndPass(\n",
    "                    pass_id,\n",
    "                    evaluator=pass_evaluator,\n",
    "                    gm=self.__gradient_machine__))\n",
    "\n",
    "event_handler分别在BeginIteration\\endIterator\\BeginPass\\EndPass去分别更新batch_id\\pass_id 等信息，在event_handler中，如果是endIterator或者EndPass对象，则打印相应的信息。\n",
    "\n",
    "### 经典例子\n",
    "前面从基础部分了解了paddlepaddle， 也从代码层面看了，如何在框架中实现一些基本的逻辑功能、参数配置等等，接下来会给出两个例子：线性回归和逻辑回归。\n",
    "\n",
    "#### 线性回归\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 0, Batch 0, Cost 10.112990\n",
      "Pass 0, Batch 1, Cost 20.137239\n",
      "Pass 0, Batch 2, Cost 114.800476\n",
      "Pass 0, Batch 3, Cost 126.498009\n",
      "Pass 1, Batch 0, Cost 7.693559\n",
      "Pass 1, Batch 1, Cost 15.439583\n",
      "Pass 1, Batch 2, Cost 89.847649\n",
      "Pass 1, Batch 3, Cost 99.887154\n",
      "Pass 2, Batch 0, Cost 5.827777\n",
      "Pass 2, Batch 1, Cost 11.802526\n",
      "Pass 2, Batch 2, Cost 70.309830\n",
      "Pass 2, Batch 3, Cost 78.941010\n",
      "Pass 3, Batch 0, Cost 4.392340\n",
      "Pass 3, Batch 1, Cost 8.991293\n",
      "Pass 3, Batch 2, Cost 55.013359\n",
      "Pass 3, Batch 3, Cost 62.445694\n",
      "Pass 4, Batch 0, Cost 3.291025\n",
      "Pass 4, Batch 1, Cost 6.822532\n",
      "Pass 4, Batch 2, Cost 43.038685\n",
      "Pass 4, Batch 3, Cost 49.448448\n",
      "Pass 5, Batch 0, Cost 2.448787\n",
      "Pass 5, Batch 1, Cost 5.153110\n",
      "Pass 5, Batch 2, Cost 33.665462\n",
      "Pass 5, Batch 3, Cost 39.201336\n",
      "Pass 6, Batch 0, Cost 1.807137\n",
      "Pass 6, Batch 1, Cost 3.871358\n",
      "Pass 6, Batch 2, Cost 26.329388\n",
      "Pass 6, Batch 3, Cost 31.117128\n",
      "Pass 7, Batch 0, Cost 1.320522\n",
      "Pass 7, Batch 1, Cost 2.890197\n",
      "Pass 7, Batch 2, Cost 20.588442\n",
      "Pass 7, Batch 3, Cost 24.734623\n",
      "Pass 8, Batch 0, Cost 0.953495\n",
      "Pass 8, Batch 1, Cost 2.141770\n",
      "Pass 8, Batch 2, Cost 16.096401\n",
      "Pass 8, Batch 3, Cost 19.691559\n",
      "Pass 9, Batch 0, Cost 0.678500\n",
      "Pass 9, Batch 1, Cost 1.573239\n",
      "Pass 9, Batch 2, Cost 12.582081\n",
      "Pass 9, Batch 3, Cost 15.703300\n",
      "Pass 10, Batch 0, Cost 0.474138\n",
      "Pass 10, Batch 1, Cost 1.143493\n",
      "Pass 10, Batch 2, Cost 9.833116\n",
      "Pass 10, Batch 3, Cost 12.546141\n",
      "Pass 11, Batch 0, Cost 0.323813\n",
      "Pass 11, Batch 1, Cost 0.820579\n",
      "Pass 11, Batch 2, Cost 7.683182\n",
      "Pass 11, Batch 3, Cost 10.044203\n",
      "Pass 12, Batch 0, Cost 0.214671\n",
      "Pass 12, Batch 1, Cost 0.579687\n",
      "Pass 12, Batch 2, Cost 6.002050\n",
      "Pass 12, Batch 3, Cost 8.059167\n",
      "Pass 13, Batch 0, Cost 0.136778\n",
      "Pass 13, Batch 1, Cost 0.401577\n",
      "Pass 13, Batch 2, Cost 4.687749\n",
      "Pass 13, Batch 3, Cost 6.482206\n",
      "Pass 14, Batch 0, Cost 0.082466\n",
      "Pass 14, Batch 1, Cost 0.271354\n",
      "Pass 14, Batch 2, Cost 3.660445\n",
      "Pass 14, Batch 3, Cost 5.227666\n",
      "Pass 15, Batch 0, Cost 0.045832\n",
      "Pass 15, Batch 1, Cost 0.177500\n",
      "Pass 15, Batch 2, Cost 2.857646\n",
      "Pass 15, Batch 3, Cost 4.228086\n",
      "Pass 16, Batch 0, Cost 0.022345\n",
      "Pass 16, Batch 1, Cost 0.111129\n",
      "Pass 16, Batch 2, Cost 2.230429\n",
      "Pass 16, Batch 3, Cost 3.430318\n",
      "Pass 17, Batch 0, Cost 0.008536\n",
      "Pass 17, Batch 1, Cost 0.065400\n",
      "Pass 17, Batch 2, Cost 1.740514\n",
      "Pass 17, Batch 3, Cost 2.792463\n",
      "Pass 18, Batch 0, Cost 0.001759\n",
      "Pass 18, Batch 1, Cost 0.035061\n",
      "Pass 18, Batch 2, Cost 1.357938\n",
      "Pass 18, Batch 3, Cost 2.281466\n",
      "Pass 19, Batch 0, Cost 0.000007\n",
      "Pass 19, Batch 1, Cost 0.016090\n",
      "Pass 19, Batch 2, Cost 1.059266\n",
      "Pass 19, Batch 3, Cost 1.871227\n",
      "Pass 20, Batch 0, Cost 0.001762\n",
      "Pass 20, Batch 1, Cost 0.005417\n",
      "Pass 20, Batch 2, Cost 0.826157\n",
      "Pass 20, Batch 3, Cost 1.541131\n",
      "Pass 21, Batch 0, Cost 0.005885\n",
      "Pass 21, Batch 1, Cost 0.000712\n",
      "Pass 21, Batch 2, Cost 0.644273\n",
      "Pass 21, Batch 3, Cost 1.274875\n",
      "Pass 22, Batch 0, Cost 0.011530\n",
      "Pass 22, Batch 1, Cost 0.000214\n",
      "Pass 22, Batch 2, Cost 0.502397\n",
      "Pass 22, Batch 3, Cost 1.059552\n",
      "Pass 23, Batch 0, Cost 0.018070\n",
      "Pass 23, Batch 1, Cost 0.002598\n",
      "Pass 23, Batch 2, Cost 0.391760\n",
      "Pass 23, Batch 3, Cost 0.884938\n",
      "Pass 24, Batch 0, Cost 0.025051\n",
      "Pass 24, Batch 1, Cost 0.006877\n",
      "Pass 24, Batch 2, Cost 0.305509\n",
      "Pass 24, Batch 3, Cost 0.742921\n",
      "Pass 25, Batch 0, Cost 0.032148\n",
      "Pass 25, Batch 1, Cost 0.012321\n",
      "Pass 25, Batch 2, Cost 0.238288\n",
      "Pass 25, Batch 3, Cost 0.627060\n",
      "Pass 26, Batch 0, Cost 0.039131\n",
      "Pass 26, Batch 1, Cost 0.018398\n",
      "Pass 26, Batch 2, Cost 0.185911\n",
      "Pass 26, Batch 3, Cost 0.532229\n",
      "Pass 27, Batch 0, Cost 0.045845\n",
      "Pass 27, Batch 1, Cost 0.024724\n",
      "Pass 27, Batch 2, Cost 0.145111\n",
      "Pass 27, Batch 3, Cost 0.454348\n",
      "Pass 28, Batch 0, Cost 0.052190\n",
      "Pass 28, Batch 1, Cost 0.031028\n",
      "Pass 28, Batch 2, Cost 0.113336\n",
      "Pass 28, Batch 3, Cost 0.390161\n",
      "Pass 29, Batch 0, Cost 0.058106\n",
      "Pass 29, Batch 1, Cost 0.037124\n",
      "Pass 29, Batch 2, Cost 0.088594\n",
      "Pass 29, Batch 3, Cost 0.337067\n",
      "Pass 30, Batch 0, Cost 0.063562\n",
      "Pass 30, Batch 1, Cost 0.042892\n",
      "Pass 30, Batch 2, Cost 0.069330\n",
      "Pass 30, Batch 3, Cost 0.292981\n",
      "Pass 31, Batch 0, Cost 0.068550\n",
      "Pass 31, Batch 1, Cost 0.048254\n",
      "Pass 31, Batch 2, Cost 0.054333\n",
      "Pass 31, Batch 3, Cost 0.256233\n",
      "Pass 32, Batch 0, Cost 0.073073\n",
      "Pass 32, Batch 1, Cost 0.053171\n",
      "Pass 32, Batch 2, Cost 0.042657\n",
      "Pass 32, Batch 3, Cost 0.225479\n",
      "Pass 33, Batch 0, Cost 0.077148\n",
      "Pass 33, Batch 1, Cost 0.057625\n",
      "Pass 33, Batch 2, Cost 0.033565\n",
      "Pass 33, Batch 3, Cost 0.199636\n",
      "Pass 34, Batch 0, Cost 0.080797\n",
      "Pass 34, Batch 1, Cost 0.061619\n",
      "Pass 34, Batch 2, Cost 0.026484\n",
      "Pass 34, Batch 3, Cost 0.177830\n",
      "Pass 35, Batch 0, Cost 0.084047\n",
      "Pass 35, Batch 1, Cost 0.065164\n",
      "Pass 35, Batch 2, Cost 0.020967\n",
      "Pass 35, Batch 3, Cost 0.159354\n",
      "Pass 36, Batch 0, Cost 0.086925\n",
      "Pass 36, Batch 1, Cost 0.068283\n",
      "Pass 36, Batch 2, Cost 0.016667\n",
      "Pass 36, Batch 3, Cost 0.143631\n",
      "Pass 37, Batch 0, Cost 0.089461\n",
      "Pass 37, Batch 1, Cost 0.071001\n",
      "Pass 37, Batch 2, Cost 0.013311\n",
      "Pass 37, Batch 3, Cost 0.130194\n",
      "Pass 38, Batch 0, Cost 0.091684\n",
      "Pass 38, Batch 1, Cost 0.073346\n",
      "Pass 38, Batch 2, Cost 0.010692\n",
      "Pass 38, Batch 3, Cost 0.118661\n",
      "Pass 39, Batch 0, Cost 0.093621\n",
      "Pass 39, Batch 1, Cost 0.075349\n",
      "Pass 39, Batch 2, Cost 0.008644\n",
      "Pass 39, Batch 3, Cost 0.108720\n",
      "Pass 40, Batch 0, Cost 0.095300\n",
      "Pass 40, Batch 1, Cost 0.077040\n",
      "Pass 40, Batch 2, Cost 0.007041\n",
      "Pass 40, Batch 3, Cost 0.100113\n",
      "Pass 41, Batch 0, Cost 0.096746\n",
      "Pass 41, Batch 1, Cost 0.078447\n",
      "Pass 41, Batch 2, Cost 0.005785\n",
      "Pass 41, Batch 3, Cost 0.092629\n",
      "Pass 42, Batch 0, Cost 0.097981\n",
      "Pass 42, Batch 1, Cost 0.079598\n",
      "Pass 42, Batch 2, Cost 0.004799\n",
      "Pass 42, Batch 3, Cost 0.086092\n",
      "Pass 43, Batch 0, Cost 0.099028\n",
      "Pass 43, Batch 1, Cost 0.080519\n",
      "Pass 43, Batch 2, Cost 0.004024\n",
      "Pass 43, Batch 3, Cost 0.080358\n",
      "Pass 44, Batch 0, Cost 0.099906\n",
      "Pass 44, Batch 1, Cost 0.081235\n",
      "Pass 44, Batch 2, Cost 0.003414\n",
      "Pass 44, Batch 3, Cost 0.075306\n",
      "Pass 45, Batch 0, Cost 0.100634\n",
      "Pass 45, Batch 1, Cost 0.081767\n",
      "Pass 45, Batch 2, Cost 0.002933\n",
      "Pass 45, Batch 3, Cost 0.070836\n",
      "Pass 46, Batch 0, Cost 0.101227\n",
      "Pass 46, Batch 1, Cost 0.082137\n",
      "Pass 46, Batch 2, Cost 0.002554\n",
      "Pass 46, Batch 3, Cost 0.066862\n",
      "Pass 47, Batch 0, Cost 0.101701\n",
      "Pass 47, Batch 1, Cost 0.082363\n",
      "Pass 47, Batch 2, Cost 0.002255\n",
      "Pass 47, Batch 3, Cost 0.063316\n",
      "Pass 48, Batch 0, Cost 0.102069\n",
      "Pass 48, Batch 1, Cost 0.082462\n",
      "Pass 48, Batch 2, Cost 0.002020\n",
      "Pass 48, Batch 3, Cost 0.060136\n",
      "Pass 49, Batch 0, Cost 0.102343\n",
      "Pass 49, Batch 1, Cost 0.082449\n",
      "Pass 49, Batch 2, Cost 0.001835\n",
      "Pass 49, Batch 3, Cost 0.057273\n",
      "Pass 50, Batch 0, Cost 0.102534\n",
      "Pass 50, Batch 1, Cost 0.082338\n",
      "Pass 50, Batch 2, Cost 0.001692\n",
      "Pass 50, Batch 3, Cost 0.054684\n",
      "Pass 51, Batch 0, Cost 0.102651\n",
      "Pass 51, Batch 1, Cost 0.082142\n",
      "Pass 51, Batch 2, Cost 0.001582\n",
      "Pass 51, Batch 3, Cost 0.052333\n",
      "Pass 52, Batch 0, Cost 0.102703\n",
      "Pass 52, Batch 1, Cost 0.081871\n",
      "Pass 52, Batch 2, Cost 0.001500\n",
      "Pass 52, Batch 3, Cost 0.050190\n",
      "Pass 53, Batch 0, Cost 0.102698\n",
      "Pass 53, Batch 1, Cost 0.081535\n",
      "Pass 53, Batch 2, Cost 0.001439\n",
      "Pass 53, Batch 3, Cost 0.048227\n",
      "Pass 54, Batch 0, Cost 0.102643\n",
      "Pass 54, Batch 1, Cost 0.081143\n",
      "Pass 54, Batch 2, Cost 0.001398\n",
      "Pass 54, Batch 3, Cost 0.046423\n",
      "Pass 55, Batch 0, Cost 0.102543\n",
      "Pass 55, Batch 1, Cost 0.080703\n",
      "Pass 55, Batch 2, Cost 0.001372\n",
      "Pass 55, Batch 3, Cost 0.044757\n",
      "Pass 56, Batch 0, Cost 0.102405\n",
      "Pass 56, Batch 1, Cost 0.080222\n",
      "Pass 56, Batch 2, Cost 0.001360\n",
      "Pass 56, Batch 3, Cost 0.043214\n",
      "Pass 57, Batch 0, Cost 0.102232\n",
      "Pass 57, Batch 1, Cost 0.079705\n",
      "Pass 57, Batch 2, Cost 0.001360\n",
      "Pass 57, Batch 3, Cost 0.041779\n",
      "Pass 58, Batch 0, Cost 0.102030\n",
      "Pass 58, Batch 1, Cost 0.079158\n",
      "Pass 58, Batch 2, Cost 0.001370\n",
      "Pass 58, Batch 3, Cost 0.040440\n",
      "Pass 59, Batch 0, Cost 0.101801\n",
      "Pass 59, Batch 1, Cost 0.078586\n",
      "Pass 59, Batch 2, Cost 0.001389\n",
      "Pass 59, Batch 3, Cost 0.039186\n",
      "Pass 60, Batch 0, Cost 0.101550\n",
      "Pass 60, Batch 1, Cost 0.077992\n",
      "Pass 60, Batch 2, Cost 0.001416\n",
      "Pass 60, Batch 3, Cost 0.038008\n",
      "Pass 61, Batch 0, Cost 0.101280\n",
      "Pass 61, Batch 1, Cost 0.077382\n",
      "Pass 61, Batch 2, Cost 0.001450\n",
      "Pass 61, Batch 3, Cost 0.036897\n",
      "Pass 62, Batch 0, Cost 0.100992\n",
      "Pass 62, Batch 1, Cost 0.076757\n",
      "Pass 62, Batch 2, Cost 0.001491\n",
      "Pass 62, Batch 3, Cost 0.035847\n",
      "Pass 63, Batch 0, Cost 0.100689\n",
      "Pass 63, Batch 1, Cost 0.076122\n",
      "Pass 63, Batch 2, Cost 0.001538\n",
      "Pass 63, Batch 3, Cost 0.034851\n",
      "Pass 64, Batch 0, Cost 0.100375\n",
      "Pass 64, Batch 1, Cost 0.075477\n",
      "Pass 64, Batch 2, Cost 0.001590\n",
      "Pass 64, Batch 3, Cost 0.033905\n",
      "Pass 65, Batch 0, Cost 0.100049\n",
      "Pass 65, Batch 1, Cost 0.074826\n",
      "Pass 65, Batch 2, Cost 0.001648\n",
      "Pass 65, Batch 3, Cost 0.033004\n",
      "Pass 66, Batch 0, Cost 0.099714\n",
      "Pass 66, Batch 1, Cost 0.074170\n",
      "Pass 66, Batch 2, Cost 0.001710\n",
      "Pass 66, Batch 3, Cost 0.032142\n",
      "Pass 67, Batch 0, Cost 0.099371\n",
      "Pass 67, Batch 1, Cost 0.073512\n",
      "Pass 67, Batch 2, Cost 0.001776\n",
      "Pass 67, Batch 3, Cost 0.031318\n",
      "Pass 68, Batch 0, Cost 0.099022\n",
      "Pass 68, Batch 1, Cost 0.072852\n",
      "Pass 68, Batch 2, Cost 0.001847\n",
      "Pass 68, Batch 3, Cost 0.030528\n",
      "Pass 69, Batch 0, Cost 0.098667\n",
      "Pass 69, Batch 1, Cost 0.072191\n",
      "Pass 69, Batch 2, Cost 0.001921\n",
      "Pass 69, Batch 3, Cost 0.029768\n",
      "Pass 70, Batch 0, Cost 0.098308\n",
      "Pass 70, Batch 1, Cost 0.071532\n",
      "Pass 70, Batch 2, Cost 0.001998\n",
      "Pass 70, Batch 3, Cost 0.029037\n",
      "Pass 71, Batch 0, Cost 0.097945\n",
      "Pass 71, Batch 1, Cost 0.070874\n",
      "Pass 71, Batch 2, Cost 0.002079\n",
      "Pass 71, Batch 3, Cost 0.028332\n",
      "Pass 72, Batch 0, Cost 0.097579\n",
      "Pass 72, Batch 1, Cost 0.070219\n",
      "Pass 72, Batch 2, Cost 0.002162\n",
      "Pass 72, Batch 3, Cost 0.027652\n",
      "Pass 73, Batch 0, Cost 0.097211\n",
      "Pass 73, Batch 1, Cost 0.069568\n",
      "Pass 73, Batch 2, Cost 0.002249\n",
      "Pass 73, Batch 3, Cost 0.026994\n",
      "Pass 74, Batch 0, Cost 0.096841\n",
      "Pass 74, Batch 1, Cost 0.068920\n",
      "Pass 74, Batch 2, Cost 0.002338\n",
      "Pass 74, Batch 3, Cost 0.026357\n",
      "Pass 75, Batch 0, Cost 0.096471\n",
      "Pass 75, Batch 1, Cost 0.068277\n",
      "Pass 75, Batch 2, Cost 0.002429\n",
      "Pass 75, Batch 3, Cost 0.025740\n",
      "Pass 76, Batch 0, Cost 0.096099\n",
      "Pass 76, Batch 1, Cost 0.067639\n",
      "Pass 76, Batch 2, Cost 0.002522\n",
      "Pass 76, Batch 3, Cost 0.025142\n",
      "Pass 77, Batch 0, Cost 0.095728\n",
      "Pass 77, Batch 1, Cost 0.067006\n",
      "Pass 77, Batch 2, Cost 0.002617\n",
      "Pass 77, Batch 3, Cost 0.024562\n",
      "Pass 78, Batch 0, Cost 0.095356\n",
      "Pass 78, Batch 1, Cost 0.066379\n",
      "Pass 78, Batch 2, Cost 0.002715\n",
      "Pass 78, Batch 3, Cost 0.023997\n",
      "Pass 79, Batch 0, Cost 0.094985\n",
      "Pass 79, Batch 1, Cost 0.065757\n",
      "Pass 79, Batch 2, Cost 0.002813\n",
      "Pass 79, Batch 3, Cost 0.023449\n",
      "Pass 80, Batch 0, Cost 0.094614\n",
      "Pass 80, Batch 1, Cost 0.065142\n",
      "Pass 80, Batch 2, Cost 0.002914\n",
      "Pass 80, Batch 3, Cost 0.022915\n",
      "Pass 81, Batch 0, Cost 0.094244\n",
      "Pass 81, Batch 1, Cost 0.064533\n",
      "Pass 81, Batch 2, Cost 0.003016\n",
      "Pass 81, Batch 3, Cost 0.022395\n",
      "Pass 82, Batch 0, Cost 0.093876\n",
      "Pass 82, Batch 1, Cost 0.063931\n",
      "Pass 82, Batch 2, Cost 0.003119\n",
      "Pass 82, Batch 3, Cost 0.021889\n",
      "Pass 83, Batch 0, Cost 0.093508\n",
      "Pass 83, Batch 1, Cost 0.063335\n",
      "Pass 83, Batch 2, Cost 0.003223\n",
      "Pass 83, Batch 3, Cost 0.021396\n",
      "Pass 84, Batch 0, Cost 0.093142\n",
      "Pass 84, Batch 1, Cost 0.062746\n",
      "Pass 84, Batch 2, Cost 0.003329\n",
      "Pass 84, Batch 3, Cost 0.020915\n",
      "Pass 85, Batch 0, Cost 0.092777\n",
      "Pass 85, Batch 1, Cost 0.062163\n",
      "Pass 85, Batch 2, Cost 0.003435\n",
      "Pass 85, Batch 3, Cost 0.020446\n",
      "Pass 86, Batch 0, Cost 0.092414\n",
      "Pass 86, Batch 1, Cost 0.061587\n",
      "Pass 86, Batch 2, Cost 0.003543\n",
      "Pass 86, Batch 3, Cost 0.019989\n",
      "Pass 87, Batch 0, Cost 0.092053\n",
      "Pass 87, Batch 1, Cost 0.061018\n",
      "Pass 87, Batch 2, Cost 0.003651\n",
      "Pass 87, Batch 3, Cost 0.019542\n",
      "Pass 88, Batch 0, Cost 0.091693\n",
      "Pass 88, Batch 1, Cost 0.060456\n",
      "Pass 88, Batch 2, Cost 0.003760\n",
      "Pass 88, Batch 3, Cost 0.019106\n",
      "Pass 89, Batch 0, Cost 0.091336\n",
      "Pass 89, Batch 1, Cost 0.059901\n",
      "Pass 89, Batch 2, Cost 0.003869\n",
      "Pass 89, Batch 3, Cost 0.018680\n",
      "Pass 90, Batch 0, Cost 0.090980\n",
      "Pass 90, Batch 1, Cost 0.059352\n",
      "Pass 90, Batch 2, Cost 0.003979\n",
      "Pass 90, Batch 3, Cost 0.018265\n",
      "Pass 91, Batch 0, Cost 0.090626\n",
      "Pass 91, Batch 1, Cost 0.058810\n",
      "Pass 91, Batch 2, Cost 0.004089\n",
      "Pass 91, Batch 3, Cost 0.017858\n",
      "Pass 92, Batch 0, Cost 0.090274\n",
      "Pass 92, Batch 1, Cost 0.058274\n",
      "Pass 92, Batch 2, Cost 0.004200\n",
      "Pass 92, Batch 3, Cost 0.017461\n",
      "Pass 93, Batch 0, Cost 0.089924\n",
      "Pass 93, Batch 1, Cost 0.057746\n",
      "Pass 93, Batch 2, Cost 0.004312\n",
      "Pass 93, Batch 3, Cost 0.017073\n",
      "Pass 94, Batch 0, Cost 0.089576\n",
      "Pass 94, Batch 1, Cost 0.057224\n",
      "Pass 94, Batch 2, Cost 0.004423\n",
      "Pass 94, Batch 3, Cost 0.016694\n",
      "Pass 95, Batch 0, Cost 0.089231\n",
      "Pass 95, Batch 1, Cost 0.056708\n",
      "Pass 95, Batch 2, Cost 0.004535\n",
      "Pass 95, Batch 3, Cost 0.016323\n",
      "Pass 96, Batch 0, Cost 0.088887\n",
      "Pass 96, Batch 1, Cost 0.056199\n",
      "Pass 96, Batch 2, Cost 0.004647\n",
      "Pass 96, Batch 3, Cost 0.015961\n",
      "Pass 97, Batch 0, Cost 0.088546\n",
      "Pass 97, Batch 1, Cost 0.055696\n",
      "Pass 97, Batch 2, Cost 0.004759\n",
      "Pass 97, Batch 3, Cost 0.015607\n",
      "Pass 98, Batch 0, Cost 0.088206\n",
      "Pass 98, Batch 1, Cost 0.055200\n",
      "Pass 98, Batch 2, Cost 0.004871\n",
      "Pass 98, Batch 3, Cost 0.015260\n",
      "Pass 99, Batch 0, Cost 0.087869\n",
      "Pass 99, Batch 1, Cost 0.054710\n",
      "Pass 99, Batch 2, Cost 0.004983\n",
      "Pass 99, Batch 3, Cost 0.014921\n"
     ]
    }
   ],
   "source": [
    "import paddle.v2 as paddle\n",
    "import numpy as np\n",
    "\n",
    "paddle.init(use_gpu=False)\n",
    "\n",
    "x = paddle.layer.data(name='x', type=paddle.data_type.dense_vector(2))\n",
    "y = paddle.layer.data(name='y', type=paddle.data_type.dense_vector(1))\n",
    "y_predict = paddle.layer.fc(input=x, size=1, act=paddle.activation.Linear())\n",
    "cost = paddle.layer.square_error_cost(input=y_predict, label=y)\n",
    "\n",
    "parameters = paddle.parameters.create(cost)\n",
    "optimizer = paddle.optimizer.Momentum(momentum=0)\n",
    "trainer = paddle.trainer.SGD(cost=cost,\n",
    "                             parameters=parameters,\n",
    "                             update_equation=optimizer)\n",
    "\n",
    "def event_handler(event):\n",
    "    if isinstance(event, paddle.event.EndIteration):\n",
    "        if event.batch_id % 1 == 0:\n",
    "            print \"Pass %d, Batch %d, Cost %f\" % (event.pass_id, event.batch_id,\n",
    "                                                  event.cost)\n",
    "    # product model every 10 pass\n",
    "    if isinstance(event, paddle.event.EndPass):\n",
    "        if event.pass_id % 10 == 0:\n",
    "            with open('params_pass_%d.tar' % event.pass_id, 'w') as f:\n",
    "                trainer.save_parameter_to_tar(f)\n",
    "\n",
    "def train_reader():\n",
    "    train_x = np.array([[1, 1], [1, 2], [3, 4], [5, 2]])\n",
    "    train_y = np.array([[-2], [-3], [-7], [-7]])\n",
    "\n",
    "    def reader():\n",
    "        for i in xrange(train_y.shape[0]):\n",
    "            yield train_x[i], train_y[i]\n",
    "\n",
    "    return reader\n",
    "\n",
    "feeding = {'x': 0, 'y': 1}\n",
    "\n",
    "trainer.train(\n",
    "    reader=paddle.batch(\n",
    "        train_reader(), batch_size=1),\n",
    "    feeding=feeding,\n",
    "    event_handler=event_handler,\n",
    "    num_passes=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 逻辑回归\n",
    "mnist的分类，算是deeplearning里面的helloworld，这里我们用logistic regression来做下逻辑回归："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import paddle.v2 as paddle\n",
    "\n",
    "with_gpu = os.getenv('WITH_GPU', '0') != '0'\n",
    "\n",
    "\n",
    "def softmax_regression(img):\n",
    "    predict = paddle.layer.fc(\n",
    "        input=img, size=10, act=paddle.activation.Softmax())\n",
    "    return predict\n",
    "\n",
    "\n",
    "def main():\n",
    "    paddle.init(use_gpu=True, trainer_count=1)\n",
    "\n",
    "    images = paddle.layer.data(\n",
    "        name='pixel', type=paddle.data_type.dense_vector(784))\n",
    "    label = paddle.layer.data(\n",
    "        name='label', type=paddle.data_type.integer_value(10))\n",
    "\n",
    "    predict = softmax_regression(images)\n",
    "    cost = paddle.layer.classification_cost(input=predict, label=label)\n",
    "    parameters = paddle.parameters.create(cost)\n",
    "\n",
    "    optimizer = paddle.optimizer.Momentum(\n",
    "        learning_rate=0.1 / 128.0,\n",
    "        momentum=0.9,\n",
    "        regularization=paddle.optimizer.L2Regularization(rate=0.0005 * 128))\n",
    "\n",
    "    trainer = paddle.trainer.SGD(\n",
    "        cost=cost, parameters=parameters, update_equation=optimizer)\n",
    "\n",
    "    lists = []\n",
    "\n",
    "    def event_handler(event):\n",
    "        if isinstance(event, paddle.event.EndIteration):\n",
    "            if event.batch_id % 100 == 0:\n",
    "                print \"Pass %d, Batch %d, Cost %f, %s\" % (\n",
    "                    event.pass_id, event.batch_id, event.cost, event.metrics)\n",
    "        if isinstance(event, paddle.event.EndPass):\n",
    "            # save parameters\n",
    "            with open('params_pass_%d.tar' % event.pass_id, 'w') as f:\n",
    "                trainer.save_parameter_to_tar(f)\n",
    "\n",
    "            result = trainer.test(reader=paddle.batch(\n",
    "                paddle.dataset.mnist.test(), batch_size=128))\n",
    "            print \"Test with Pass %d, Cost %f, %s\\n\" % (\n",
    "                event.pass_id, result.cost, result.metrics)\n",
    "            lists.append((event.pass_id, result.cost,\n",
    "                          result.metrics['classification_error_evaluator']))\n",
    "\n",
    "    trainer.train(\n",
    "        reader=paddle.batch(\n",
    "            paddle.reader.shuffle(paddle.dataset.mnist.train(), buf_size=8192),\n",
    "            batch_size=128),\n",
    "        event_handler=event_handler,\n",
    "        num_passes=50)\n",
    "\n",
    "    best = sorted(lists, key=lambda list: float(list[1]))[0]\n",
    "    print 'Best pass is %s, testing Avgcost is %s' % (best[0], best[1])\n",
    "    print 'The classification accuracy is %.2f%%' % (100 - float(best[2]) * 100)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
